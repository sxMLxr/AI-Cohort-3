{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2+CeQwYkmrTdio6EIouGQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/ner_zetas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx25GXPCVQ4g",
        "outputId": "95fd6379-465d-4dde-cdd0-92f306c3bd86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import string\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from collections import defaultdict\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "PUNCTUATION = set(string.punctuation)\n",
        "entity_counts = defaultdict(int)\n",
        "# Regular expressions for undesired patterns\n",
        "UNDESIRED_PATTERNS = [r\"['\\\"]\\w+$\"]"
      ],
      "metadata": {
        "id": "5bcKhDqdooDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # This is where the preprocess step\n",
        "    # Remove undesired patterns\n",
        "    for pattern in UNDESIRED_PATTERNS:\n",
        "        text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    # Replace newline characters and carriage returns with a space\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\n",
        "    # Lowercase - lowercasing improves tokenizing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove punctuation\n",
        "    words = wordpunct_tokenize(text)\n",
        "    words = [word for word in words if word not in PUNCTUATION]\n",
        "\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in STOP_WORDS]\n",
        "\n",
        "    # Convert numbers to words\n",
        "    words = [num2words(word) if word.isdigit() else word for word in words]\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "44vWdxAgZd6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we connect our document store whether that is google drive or some other db\n",
        "# Insert your text here\n",
        "text = \"\"\"<YOUR TEXT HERE>\"\"\"\n",
        "\n",
        "preprocessed_text = preprocess_text(text)\n",
        "print(preprocessed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5P9HupnV-ex",
        "outputId": "09391bb8-3a3e-45d9-fa0f-d9850c9fbcef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "today rapidly evolving technological landscape artificial intelligence ai tools leaving indelible mark various sectors including government reactions government agencies leaders towards emerging ai tools mixed ranging outright prohibition cautious exploration zcore group practical experience working federal customers extensive prototyping depth research leads us believe ai tools present valuable opportunity government agencies high time agencies recognize potential several compelling reasons consumer technology world already integrating ai products chris sales recently said enhancing existing software creating new experiences ground result public expectations high quality experiences services increasingly influenced advantages ai brings government agencies must intentional leveraging ai improve customer experience avoid widening gap public expectations deliver ai sets apart emerging technologies demonstrating potential bring value government technologies like blockchain received significant hype yet prove worth solving real government customer experience challenges hand new ai tools large language models llms ), already shown impressive alignment common government problems llms excel refining organizing large amounts unstructured data challenge faced nearly every government agency recent advancements particularly llm performance made ai practical exciting field enhancing user experiences software capabilities unlike emergent technologies like blockchain ai integration existing systems applications simpler less intrusive thanks mature tools resources available responsibly effectively use ai tools agencies must focus solving real problems real people whether internal staff grappling unstructured data public seeking plain language answers benefits eligibility agencies evaluate potential ai tools based ability enhance customer experiences criterion guide technology selection application strategies partner collaborations still unresolved policy implementation issues surrounding ai tools government taking pragmatic approach apply ai tools customer experience problems presents golden opportunity improve public services although challenges exist exploring ai tools thoughtfully applied government services yield valuable results #### potential use cases ai government enhanced search experience llms garnered attention ability provide natural familiar search experience compared traditional web search semantic search understands user query within context coupled llms summarization question answering capabilities drastically simplify shorten user experience finding understanding important information government agencies leverage ai powered semantic search improve discoverability accessibility critical information setting high standard others follow insights messy data llms offer agencies insights unstructured data transforming information ways possible tools unstructured data poses challenge almost every government agency llms extract meaning piece text thereby increasing value large data systems carefully applying llm tools specific data challenges agencies gain valuable insights customers system usage prioritize changes provide value public plain language translations despite advancements plain language requirements nuances benefit eligibility process documentation regulations remain trapped complex legal language llms well suited translate style tone text converting understandable plain language combined summarization capabilities llms aid individuals grasping federal laws programs apply interactive nature llms allows clarifying questions updated responses making government services benefits accessible promoting efficient inclusive public service delivery call center representative support llms serve intelligent assistants call center representatives providing valuable information based agency policies documentation representatives use llm tools look information callers tailoring responses caller specific needs ensuring accuracy remains challenge llms boost efficiency call center representatives improve overall customer experience expanding beyond text text based ai improved tools focused audio images made significant strides combined llms audio tools convert speech text vice versa making interactions accessible wider range devices similarly ai models focused images computer vision generate text descriptions aiding digitizing paper records advancing modernization goals #### ai ready inserting deliberate irrelevant entity represent typo boeing ai tools benefits essential understand limitations ai yet ready make actual decisions behalf people crucial note ai models comprehend text context humans doinsen inherent ethical judgment use ai tools decision making processes handled extreme care clear guidelines human intervention furthermore ai advanced leaps bounds still requires expert guidance implemented effectively agencies need build expertise ai invest right infrastructure successfully apply ai technologies conclusion ai tools particularly llms bring incredible benefits government services implementation requires careful thought rigorous testing ongoing monitoring ensure meet user needs deliver public value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "entity_counts = defaultdict(int)\n",
        "\n",
        "for sent in nltk.sent_tokenize(text):\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            entity = ' '.join(c[0] for c in chunk)\n",
        "            entity_counts[entity] += 1\n",
        "\n",
        "for entity, count in entity_counts.items():\n",
        "    print(f\"{entity}({count})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpn-fu4EWSWL",
        "outputId": "e1826c75-1b4a-464f-bfb8-02b7300a9df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zCore Group(1)\n",
            "AI(13)\n",
            "Chris(1)\n",
            "Large Language Models(1)\n",
            "LLMs(9)\n",
            "LLM(3)\n",
            "Semantic(1)\n",
            "Plain(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary into a pandas DataFrame\n",
        "df = pd.DataFrame.from_dict(entity_counts, orient='index', columns=['Count'])\n",
        "\n",
        "# If you want to reset the index and have the entities as a separate column:\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={\"index\": \"Entity\"}, inplace=True)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwcXt4q-uF8E",
        "outputId": "d761c640-491c-4cfb-929a-3e731b4cbaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Entity  Count\n",
            "0            zCore Group      1\n",
            "1                     AI     13\n",
            "2                  Chris      1\n",
            "3  Large Language Models      1\n",
            "4                   LLMs      9\n",
            "5                    LLM      3\n",
            "6               Semantic      1\n",
            "7                  Plain      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get file location from user\n",
        "file_location = input(\"Enter file location and name: \")\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(file_location, index=False)"
      ],
      "metadata": {
        "id": "T9TlRMipvsGo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}