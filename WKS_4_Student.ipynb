{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/WKS_4_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYeUmQYYCYF-"
      },
      "source": [
        "# **Workshop 4**\n",
        "\n",
        "In this workshop, you'll be exploring decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfGe2uqsCYF_"
      },
      "source": [
        "# 0) Loading Data and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm_NlV8PCYF_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# we're using the Diabetes dataset from sklearn.datasets\n",
        "from sklearn import datasets\n",
        "# Remember you have to run this cell block before continuing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL_TkN_LCYGA"
      },
      "source": [
        "## Important Note about Non-Determinism (Randomness)\n",
        "Here on foward we're going to be using a lot of methods that have some randomness in their output. To address this, most sklearn libraries/functions allow you to set a `random_state` parameter, which allows the methods to have consistent output each time you run them. Make sure to set `random_state=random_seed` so we can reproduce and check your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm2hSfWwCYGA"
      },
      "outputs": [],
      "source": [
        "# set a seed for reproducibility\n",
        "random_seed = 25\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5t3cjdHCYGA"
      },
      "source": [
        "### Data\n",
        "\n",
        "We'll be using the iris dataset as a toy example to learn decision trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLNUfOsyCYGA"
      },
      "outputs": [],
      "source": [
        "# Read the iris dataset and translate to pandas dataframe\n",
        "iris_sk = datasets.load_iris()\n",
        "# Note that the \"target\" attribute is species, represented as an integer\n",
        "iris_data = pd.DataFrame(data= np.c_[iris_sk['data'], iris_sk['target']],columns= iris_sk['feature_names'] + ['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrWFuWO1CYGA"
      },
      "source": [
        "# 1) Training/Testing Split (Follow)\n",
        "\n",
        "Sklearn has a neat little library for easily splitting your data into training/testing: [Train Test Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "\n",
        "**Use this to split your data where `test_data_fraction` is the fraction of data that will be test data**. You will have to reshape your dataframes to split them between features and your target before you put it into `train_test_split`\n",
        "\n",
        "Make sure that in any non-deterministic process, to set the `random_state=random_seed`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qkajRrSCYGB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# The fraction of data that will be test data\n",
        "test_data_fraction = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qesOiob-CYGB"
      },
      "outputs": [],
      "source": [
        "# Goal: Split your data into features(X) and labels (Y)\n",
        "# Grab all the columns except the last one\n",
        "iris_features = iris_data.iloc[:,0:-1]\n",
        "\n",
        "#Grab the last column\n",
        "iris_labels = iris_data[\"target\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq1ibC0kCYGB"
      },
      "outputs": [],
      "source": [
        "# Goal: Generate the X/Y train/test datasets (4 total)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(iris_features, iris_labels, test_size=test_data_fraction,  random_state=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjRhzkvJCYGB"
      },
      "outputs": [],
      "source": [
        "print(\"Training Shape\")\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "\n",
        "print(\"Testing Shape\")\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n",
        "\n",
        "# Making sure the shapes line up:\n",
        "# You should have 4 features in the X data and 1 class label in the Y\n",
        "assert(X_test.shape[0]==np.ceil(test_data_fraction*iris_features.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07dGBl3BCYGB"
      },
      "source": [
        "# 2) Classification with Decision Trees (Group)\n",
        "\n",
        "Now that you know how to split your data into training and test data, you can now make a [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Use sklearn to create a decision tree classifier for the iris data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeuonJtiCYGB"
      },
      "source": [
        "## 2.1) Gini Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AVBRS5XCYGB"
      },
      "source": [
        "Make a decision tree classifier using the Gini Index.\n",
        "\n",
        "As always, rember your `random_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9-OcmHoCYGC"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import plot_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6UPB-sRCYGC"
      },
      "outputs": [],
      "source": [
        "# Goal: create a decision tree using gini index\n",
        "gini_tree = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Oi4bVKaICYGC"
      },
      "outputs": [],
      "source": [
        "# We can plot the resulting tree\n",
        "plt.figure(figsize=(20,10))\n",
        "_ = plot_tree(gini_tree, feature_names=iris_features.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YNJ0-LiCYGC"
      },
      "source": [
        "**Interpreting Trees**: Each node in the output tree has a few details:\n",
        "\n",
        "1. The attribute split (and split point for continuous values). Note the right is \"false\" (greater than) and left is \"true\" (less than or equal to).\n",
        "2. The GINI split value for the attribute selected.\n",
        "3. The number of *training* samples in the node before it was split.\n",
        "4. The number of instances of each class value in the node (this was used to calculate the GINI index)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E3CW8QGCYGC"
      },
      "source": [
        "First, let's calculate the **Training Accuracy**. Remember this is the \"easy\" part, since this is the data the classifier was trained on.\n",
        "\n",
        "To do so, we need 2 library functions:\n",
        "* [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'s predict method\n",
        "* [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNVEX065CYGC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Goal: Calculate training accuracy score for the gini tree. What should it be?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ-Idu3UCYGC"
      },
      "source": [
        "Now, calculate the **Test Accuracy**, using the test dataset. Do you expect this to be as good as the training accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj6nThFQCYGD"
      },
      "outputs": [],
      "source": [
        "# Goal: Calculate test accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2JH9T2TCYGD"
      },
      "source": [
        "## 2.1) Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_JbYfy0CYGD"
      },
      "source": [
        "Repeat the same procedure as 2.1, but using \"entropy\" instead of \"gini\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnBKoXjjCYGD"
      },
      "outputs": [],
      "source": [
        "# Goal: Create tree with entropy based splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTaVOHYdCYGD"
      },
      "source": [
        "What differences do you observe between the structure of the GINI and entropy trees?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caC39Uy_CYGD"
      },
      "outputs": [],
      "source": [
        "# Calculate the test accuracy for the entropy tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWjDigdCCYGD"
      },
      "source": [
        "Which type of tree would be better suited for this data?  How did you come to this determination?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsUWsih7CYGD"
      },
      "source": [
        "# 3) Building Trees & Interpretation (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj7X_bE6CYGD"
      },
      "source": [
        "Now, we're going to shift from using the toy iris data back to our wine data.\n",
        "\n",
        "**Your goal is to create a set of *human-interpretable* rules to tell wines apart**. \n",
        "\n",
        "You will:\n",
        "\n",
        "1. Make a decision tree using the wine dataset. Make sure to take note of the accuracy of the decision tree.\n",
        "2. Write down a set of notes/\"flowchart\" in plain language that you could give to a field tester if they wanted to classify wine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YOomY-dCYGD"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "# Read the wine dataset and translate to pandas dataframe\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "wine_sk = datasets.load_wine()\n",
        "\n",
        "# Make sure data is in the same range\n",
        "wine_sk.data = MinMaxScaler().fit_transform(wine_sk.data)\n",
        "\n",
        "# Note that the \"target\" attribute is species, represented as an integer\n",
        "wine_data = pd.DataFrame(data= np.c_[wine_sk['data'], wine_sk['target']],columns= wine_sk['feature_names']+['target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FynwjREZCYGD"
      },
      "source": [
        "### Building the Tree\n",
        "The fraction of data that should be test data should be 20%.\n",
        "\n",
        "As always, use `random_seed=25` for all of your code seeds to make sure your ouput is reproducable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaEPUeaeCYGD"
      },
      "outputs": [],
      "source": [
        "np.random.seed(random_seed)\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_data_fraction = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJL1_-xiCYGD"
      },
      "outputs": [],
      "source": [
        "# Goal: Split the dataset into features and class labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCNX8GRPCYGE"
      },
      "outputs": [],
      "source": [
        "# Goal: split the X and Y datasets into training and testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKpHDfEcCYGE"
      },
      "source": [
        "Below, write code to train a decision tree, plot it, and calculate its accuracy.\n",
        "\n",
        "**Note**: You should use the parameter `max_depth=2` to constrain the size and complexity of your decision tree. Remember this is a form of *pre-pruning*.\n",
        "\n",
        "**Hint**: You may need to right-click and download the figure in order to view it. One some versions, you may need to shift-right-click.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKoK9B_UCYGE"
      },
      "outputs": [],
      "source": [
        "# Goal: Train a decision tree, plot it, and calculate its accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cMUkpD3CYGE"
      },
      "source": [
        "### Writeup\n",
        "\n",
        "Write up your results as if you were giving them as a note sheet to a field tester who wanted to classify wine.\n",
        "\n",
        "For example:\n",
        "1. If a wine has a color_intensity above XX, it's usually a XX wine.\n",
        "2. If a wine has a favanoids level below XX and a proline level above XX, it's either a XX or YY wine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CI0pjjFCYGE"
      },
      "source": [
        "**Put writeup here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0eNkz61CYGE"
      },
      "source": [
        "# 4) Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPvA7mzCYGE"
      },
      "source": [
        "## 4.1 More Pre-Pruning (Follow)\n",
        "Now, let's take a closer and more critical look at the above decision tree from the wine dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVLR_QuKCYGE"
      },
      "source": [
        "In our prior model, we set the `max_depth=2`. Play around with different values for `max_depth`. How does the training/test accuracy change? What is the maximum depth of the tree (e.g. no pre-pruning)? For each depth, do you think that the model is general enough, or do you think that it could possibly be overfitting? If so, what splits/leaves do you think could be overfitting? If not, why? **Discuss and explain your reasoning here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAW372zWCYGE"
      },
      "source": [
        "**Capture your discussion below**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnslfZaBCYGE"
      },
      "source": [
        "### Min Impurity Decrease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnDx3sliCYGE"
      },
      "source": [
        "Read the documentation for [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and take note of the various params that can be used for pre-pruning. We will go in detail on `min_impurity_decrease` together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpicIWSSCYGE"
      },
      "outputs": [],
      "source": [
        "impurity_decrease = 0.05\n",
        "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed, min_impurity_decrease=impurity_decrease).fit(X=X_train, y=Y_train)\n",
        "plt.figure(figsize=(20,10))\n",
        "_ = plot_tree(gini_tree, feature_names=wine_features.columns)\n",
        "print(f\"Accuracy with Impurity Decrease {impurity_decrease}\")\n",
        "print(f'Train Accuracy: {accuracy_score(Y_train, gini_tree.predict(X_train))}')\n",
        "print(f'Test Accuracy: {accuracy_score(Y_test, gini_tree.predict(X_test))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5i-FDWwCYGE"
      },
      "outputs": [],
      "source": [
        "# However, if you go too far you end up decrease both the training and test accuracy\n",
        "impurity_decrease = 0.1\n",
        "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed, min_impurity_decrease=impurity_decrease).fit(X=X_train, y=Y_train)\n",
        "plt.figure(figsize=(20,10))\n",
        "_ = plot_tree(gini_tree, feature_names=wine_features.columns)\n",
        "print(f\"Accuracy with Impurity Decrease {impurity_decrease}\")\n",
        "print(f'Train Accuracy: {accuracy_score(Y_train, gini_tree.predict(X_train))}')\n",
        "print(f'Test Accuracy: {accuracy_score(Y_test, gini_tree.predict(X_test))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFockC5kCYGF"
      },
      "source": [
        "There are other methods for pre-pruning, but we won't explore them today.\n",
        "Read the documentation for [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and take note of the various params that can be used for pre-pruning. These are\n",
        "\n",
        "* `max_depth` (already covered)\n",
        "* `min_samples_split`\n",
        "* `min_samples_leaf`\n",
        "* `max_features`\n",
        "* `max_leaf_nodes`\n",
        "* `min_impurity_decrease` (already covered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bWVtO_8CYGF"
      },
      "source": [
        "## 4.2 Post Pruning [Cost Complexity Pruning] (Follow)\n",
        "\n",
        "Sklearn implements a more advanced form of post-pruning that we haven't covered in class called [cost complexity pruning (CCP)](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html). The idea is essentially a more complex/robust form of what we've talked about - seeing if estimated error improves after pruning certain nodes after the tree is built. The amount of the tree pruned is controlled by the paramater `ccp_alpha`. \n",
        "\n",
        "Try some different values of `ccp_alpha` in the range (0,0.3) to see their effect on the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0gF_OC1CYGF"
      },
      "outputs": [],
      "source": [
        "ccp_alpha = 0.1\n",
        "\n",
        "# Goal: Train a deicsion tree with gini index with the specified ccp_alpha\n",
        "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed, ccp_alpha=ccp_alpha).fit(X=X_train, y=Y_train)\n",
        "train_acc_value = accuracy_score(Y_train, gini_tree.predict(X_train))\n",
        "test_acc_value = accuracy_score(Y_test, gini_tree.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy with ccp_alpha={ccp_alpha}\")\n",
        "print(f'Train Accuracy: {accuracy_score(Y_train, gini_tree.predict(X_train))}')\n",
        "print(f'Test Accuracy: {accuracy_score(Y_test, gini_tree.predict(X_test))}')\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "_ = plot_tree(gini_tree, feature_names=wine_features.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMoGLj9GCYGF"
      },
      "source": [
        "## 4.3 Finding the Best Alpha (Group)\n",
        "\n",
        "Of course, manually trying different alphas and seeing if the accuracy increases is highly inefficent. We can write code to automate this: trying different values of the `ccp_alpha` paramater, and seeing what gives us the best result.\n",
        "\n",
        "**Write a function that will iteratively built decision trees different values of alpha, and report back both the training and testing accuracy for each alpha value.** You'll then plot these values and determine which alpha leads to the best performing decision tree.\n",
        "\n",
        "This process of iterating over paramaters to find the best one is called *hyperparmeter tuning*, which we will cover in later lectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAxP7pGQCYGF"
      },
      "outputs": [],
      "source": [
        "def get_accuracy_list(minimum,maximum,step):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        minimum: the lowest possible value for alpha\n",
        "        maximum: the highest possible value for alpha\n",
        "        step: the step size/how much we increment alpha each try\n",
        "        \n",
        "    Output: \n",
        "        alpha_values: a list containing all of the alpha values tried\n",
        "        train_accuracy: a list containing the training accuracy for a model trained with each alpha in alpha_values\n",
        "        train_accuracy: a list containing testing accuracy for a model trained with each alpha in alpha_values\n",
        "    \n",
        "    \n",
        "    Assume the model is trained using the gini index.\n",
        "    \"\"\"\n",
        "    \n",
        "    # np.arange generates a list that starts at minimum, ends at maximum, and increments by step\n",
        "    alpha_values = np.arange(minimum,maximum,step)\n",
        "    \n",
        "    #two lists to hold our accuracy\n",
        "    train_acc = []\n",
        "    test_acc = []\n",
        "    \n",
        "    return alpha_values, train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kz4qS4FCYGF"
      },
      "outputs": [],
      "source": [
        "# Get our accuracy\n",
        "alpha_values, train_acc, test_acc = get_accuracy_list(0,0.07,0.005)\n",
        "\n",
        "# Plot our results\n",
        "plt.plot(alpha_values, train_acc, color='blue')\n",
        "plt.plot(alpha_values, test_acc, color='red')\n",
        "plt.xlabel(\"Post Pruning Alpha\")\n",
        "_ = plt.ylabel(\"Accuracy (Blue=Train; Red=Test)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRzuOsfTCYGF"
      },
      "source": [
        "1. How does training accuracy change as the alpha value changes? Why?\n",
        "2. How does the testing accuracy change as the alpha value chanegs? Why?\n",
        "3. Knowing this, what value would you choose for alpha for the final model? Why? Plot the final decision tree with your alpha selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv1sM__BCYGF"
      },
      "source": [
        "**Discuss here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQp_DQNwCYGF"
      },
      "outputs": [],
      "source": [
        "# Plot your tree here with the best alpha value"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}