{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/WKS7_Student_tues_nau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohSsvxj3oKY"
      },
      "source": [
        "# Workshop 7: Bayes Rule Rules\n",
        "\n",
        "In this workshop, we'll be looking at how to use Naive Bayes and Bayes Nets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfpOZpT36JR",
        "outputId": "35f88687-22fc-41d0-cb83-e10217bea90d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Rdl7Os3oKZ"
      },
      "source": [
        "# 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "uwRsl8_y3oKZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "# set a seed for reproducibility\n",
        "random_seed = 25\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDRXoMEM3oKa"
      },
      "source": [
        "# 1) Naive Bayes Spam Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AekfdRJp3oKa"
      },
      "source": [
        "One historical use of Naive Bayes is to try and detect [spam emails](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering). \n",
        "\n",
        "In this exercise, you will be using dataset that of emails from the [Enron Corporation](https://en.wikipedia.org/wiki/Enron_Corpus), an accounting firm that [went bankrupt in 2001 due to an accounting scandal](https://en.wikipedia.org/wiki/Enron_scandal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUnQ3Kn3oKa"
      },
      "source": [
        "## 1.1) Exploring the Data (Follow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J0b-zYmP3oKa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/AI ACADEMY/2 - Data Mining/7- Week 7/WKS7_Student/enron_emails.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "X-yYqAY23oKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50117b2-a79b-42fa-e94f-14d4ab629fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham     3672\n",
            "spam    1499\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Ham is a legitimate email, while spam is unwanted\n",
        "# Let's look at our distribution of spam and ham emails\n",
        "email_counts = df.label.value_counts()\n",
        "print(email_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunYeUfS3oKb"
      },
      "source": [
        "Keeping with the theme of meat products, some researchers call emails that are *not spam*, **ham**. \n",
        "\n",
        "To sum up: a **ham** email is a legitimate email, while a **spam** email is unwanted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "usZGaAqi3oKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bbce822-6708-40a1-9b24-51a34240f1ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     3672\n",
              "spam    1499\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Let's look at our distriubtion of spam and ham emails\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfmrjIe3oKb"
      },
      "source": [
        "Let's explore some of the ham emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "f-bzWi7k3oKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9581dd71-826c-4ef7-f0e9-0abb8cf37b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: ehronline web address change\n",
            "this message is intended for ehronline users only .\n",
            "due to a recent change to ehronline , the url ( aka \" web address \" ) for accessing ehronline needs to be changed on your computer . the change involves adding the letter \" s \" to the \" http \" reference in the url . the url for accessing ehronline should be : https : / / ehronline . enron . com .\n",
            "this change should be made by those who have added the url as a favorite on the browser .\n"
          ]
        }
      ],
      "source": [
        "print(df[df[\"label\"]==\"ham\"].text.iloc[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZPf3_D3oKc"
      },
      "source": [
        "And now the spam emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HcYfLy5f3oKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6697534e-51a2-42ae-9ce2-1d919fc36c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: back\n",
            "emile (\n",
            "the cablefilterz will allow you to receive\n",
            "all the channels that you order with your remote control ,\n",
            "payperviews , axxxmovies , sport events , special - events !\n",
            "http : / / www . 8006 hosting . com / cable /\n",
            "avocation , despoil .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df[df[\"label\"]==\"spam\"].text.iloc[18])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4358Uhc3oKc"
      },
      "source": [
        "Try exploring different emails by changing the index in the lines above. **What common traits do you notice accross the ham emails? The spam emails?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Ckb1nD3oKc"
      },
      "source": [
        "## 1.2) Bag of Words (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ52GomW3oKc"
      },
      "source": [
        "Last week we used tf-idf to represent words as feature vectors. However, sometimes simpler methods work just as well (if not better). For this, we'll be using the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation of a piece of text, which is much more interpretable than tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZerWdYep3oKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9f08ad-2edf-4d9c-8a71-b1ec988448b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 9)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvnXPhR3oKd"
      },
      "source": [
        "The CountVectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how many times word `m` appears in document `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "5lUP3XGj3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efc0381-355c-4ae7-8776-1fe89136b6cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "AMYkf9z53oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55ed762-2921-450a-d6e8-1f554b8e55e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F87S6aFI3oKd"
      },
      "source": [
        "A more interpretable view..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iJtkLpYz3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0c5ff9-8c2d-4fee-ac60-c66c03a0891b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first document.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 1,\n",
              " 'is': 1,\n",
              " 'one': 0,\n",
              " 'second': 0,\n",
              " 'the': 1,\n",
              " 'third': 0,\n",
              " 'this': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "print(corpus[0])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "XtSNslSR3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3e5373-cb78-46f2-db6b-b738954380a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This document is the second document.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 2,\n",
              " 'first': 0,\n",
              " 'is': 1,\n",
              " 'one': 0,\n",
              " 'second': 1,\n",
              " 'the': 1,\n",
              " 'third': 0,\n",
              " 'this': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "print(corpus[1])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1PaPk03oKe"
      },
      "source": [
        "Now, if you want to vectorize new data (e.g. test data), then you use the `.transform` function. If the vectorizer encounters a word it hasn't seen before, it will simply ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "bKEga4MD3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1522025f-b675-4049-fbb5-1eb306096878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "vectorizer.transform([\"This is the coolest document\"]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp85K6mp3oKe"
      },
      "source": [
        "# 1.3) Building and Running the Model (Group)\n",
        "\n",
        "Now that you have all the required tools, build a **Naive Bayes Classifier** and evaluate it on a train and test set. In this instance, [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier, which is most useful for discrete features that use frequency counts (e.g. a bag of words vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-y-VAl3y3oKe"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "e496K1us3oKe"
      },
      "outputs": [],
      "source": [
        "# Create training and test splits - 20% split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QtaDlE1I3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize on your training data using BoW\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xxhRt-pN3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "7f361c0b-5d0d-4561-b62c-914c83dd7352"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Fit the classifier below\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "mMuvREFq3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize your test data using transform and then predict the test data\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "y_pred = clf.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "esP-NBkG3oKe"
      },
      "outputs": [],
      "source": [
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "37vUPoIV3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b4d362-c0b3-4402-83f7-bed7da2e2339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[728   6]\n",
            " [ 14 287]]\n"
          ]
        }
      ],
      "source": [
        "# Print a confusion matrix using confusion_matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7VPtAci03oKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec2e376-89ed-48bf-fd6c-c44f55bef6f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.99      0.99       734\n",
            "        spam       0.98      0.95      0.97       301\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.97      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print a classification report using classification_report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from Lori K:\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=random_seed)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(train.text)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X,train.label_num)\n",
        "test_vecs = vectorizer.transform(test.text)\n",
        "predictions = clf.predict(test_vecs)\n",
        "confusion_matrix(test.label_num,predictions)\n",
        "print(classification_report(test.label_num,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DDWD1BhVyLJ",
        "outputId": "e7242684-0681-45b1-ceda-da5ed4856882"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       734\n",
            "           1       0.98      0.96      0.97       301\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.98      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ml1Cy23oKf"
      },
      "source": [
        "## 1.4) Exploring Important Words (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksJ4daT3oKf"
      },
      "source": [
        "Before you start, predict what words might be more predictive of SPAM or HAM. Make a list below of 5 words you think will be very _predictive_ of an email being SPAM, and 5 words that are predictive of being HAM. Remember this is an office email database from Enron in the 1990s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_S81Onz3oKf"
      },
      "source": [
        "Words the will predict SPAM (junk emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x\n",
        "\n",
        "Words the will predict HAM (real emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWA5Gk4B3oKf"
      },
      "source": [
        "**Technical Note: Log Probabilities**: \n",
        "\n",
        "When using probabilistic methods with large datasets, sometimes you get features with extremely small probabilities (e.g. $10^{-10}$). \n",
        "\n",
        "This becomes a problem, because computers aren't really good at doing operations with numbers at this scale. Therefore, in most systems, operations are done on the *log* of the probabilities. \n",
        "\n",
        "This makes calculations much more managable (e.g. $\\log(10^{-10})=-10$). As an added bonus, due to log rules ($log(ab)=log(a)+log(b)$), all multiplications turn into additions, which are easier for the computer.\n",
        "\n",
        "Some general rules of thumb: **the closer to zero a log prob is, the more probabable it is**, and **each time a log prob decreases by one, it's an order of magnitude less probable**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdS2iLHz3oKq"
      },
      "source": [
        "`feature_log_probs` gives us the log probabilities for each word. In notation, each of these are $P(word | class)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "bU_k5dga3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258e6722-e860-4606-e582-3039c5431073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -5.75023447,  -5.71398455, -11.28296498, ..., -13.07472445,\n",
              "       -13.07472445, -13.07472445])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# Given that a message is ham, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "A-a_XpxM3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78abafc5-a19f-410e-9846-da44520c5009"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -6.23363088,  -7.07284789,  -9.8691907 , ..., -11.74099287,\n",
              "       -11.74099287, -11.74099287])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Given that a message is SPAM, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub__FBPq3oKq"
      },
      "source": [
        "This code will sort all the words by log probability, so that all of the most probable words show up first..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ULJwZZO03oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd966f2-efe1-4b51-fbf1-b6a7555aa96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the' 'to' 'and' ... 'brewer' 'breveffo' 'cima']\n",
            "['the' 'to' 'ect' ... 'luge' 'lugging' 'ikoogybhmxdc']\n"
          ]
        }
      ],
      "source": [
        "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
        "spam_words = np.array(vectorizer.get_feature_names_out())[spam_args]\n",
        "spam_words = np.flip(spam_words)\n",
        "print(spam_words)\n",
        "\n",
        "\n",
        "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
        "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
        "ham_words = np.flip(ham_words)\n",
        "print(ham_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ZaUKsYJk3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf51e60-8c73-4dc2-ee1a-ad2e49e16c76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['the', 'to', 'and', 'of', 'in', 'you', 'for', 'this', 'is', 'your'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "spam_words[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "wIrC07iQ3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782d3fff-9e06-421f-b617-6e17b69f7473"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['the', 'to', 'ect', 'for', 'and', 'hou', 'enron', 'subject', 'on',\n",
              "       'of'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "ham_words[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fweB7UQ3oKq"
      },
      "source": [
        "However, a more useful way to look at the data is to look at the *ratios* of the probabilities for a given word. For example, if we have the word \"free\":\n",
        "\n",
        "*If an email is spam, there is a 50% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|spam)=0.5$\n",
        "\n",
        "*If an email is ham, there is a 10% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|ham)=0.1$\n",
        "\n",
        "*The ratio*\n",
        "\n",
        "$P(free|spam)/P(free|ham)=5$\n",
        "\n",
        "This means that the word *free* is 5x as more likely to show up in spam messages compared to ham messages. So, we can use this to calculate and sort for words that are proportionally more present in spam emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "2AFQz_xr3oKq"
      },
      "outputs": [],
      "source": [
        "# Since we're operating on logs, division turns into subtraction\n",
        "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
        "spam_ham_args = np.argsort(log_odds)\n",
        "spam_ham_words = np.array(vectorizer.get_feature_names_out())[spam_ham_args]\n",
        "spam_ham_words = np.flip(spam_ham_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13UPxtBZ3oKr"
      },
      "source": [
        "Here's some of the \"spammiest\" words..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "mtmlqiOA3oKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d24632e-8eda-416a-b360-62ca7cd36a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['td', 'nbsp', 'pills', 'width', 'computron', 'br', 'font', 'href',\n",
              "       'viagra', 'height', 'xp', 'src', '2004', 'cialis', 'soft', 'meds',\n",
              "       'paliourg', 'php', 'voip', 'drugs', 'oo', 'valign', 'bgcolor',\n",
              "       'biz', 'hotlist', 'moopid', 'div', 'photoshop', 'mx', 'img',\n",
              "       'knle', 'pharmacy', 'gr', 'intel', 'corel', 'prescription', 'iit',\n",
              "       'demokritos', 'rolex', 'xanax', 'macromedia', 'dealer',\n",
              "       'uncertainties', 'valium', 'htmlimg', 'darial', '000000',\n",
              "       '0310041', 'lots', 'projections', 'jebel', 'adobe', 'rnd', 'color',\n",
              "       'alt', '161', 'colspan', 'pain', 'readers', 'rx', 'canon',\n",
              "       'export', 'draw', 'fontfont', 'gra', 'speculative', '1226030',\n",
              "       'gold', 'pro', 'logos', 'wi', 'toshiba', 'china', '1933', 'spam',\n",
              "       'vicodin', 'itoy', 'viewsonic', 'ooking', '1618', 'cellpadding',\n",
              "       'weight', 'hewlett', '4176', 'pill', 'robotics', 'soma',\n",
              "       'resellers', '8834464', '8834454', 'apc', 'intellinet', 'aopen',\n",
              "       'iomega', 'enquiries', 'customerservice', 'targus', 'packard',\n",
              "       'tr', 'uae', 'dealers', 'spain', 'nomad', '1934', 'drug', 'muscle',\n",
              "       'abdv', 'zonedubai', 'eogi', 'aeor', 'doctors', 'inherent',\n",
              "       'wysak', 'emirates', 'cheap', 'health', 'border', 'illustrator',\n",
              "       'hottlist', 'oem', 'apple', 'ffffff', 'ce', 'verdana', 'sex',\n",
              "       'gif', 'resuits', 'graphics', 'mining', 'studio', 'differ',\n",
              "       'materia', 'predictions', 'arial', 'waste', 'cellspacing', 'yap',\n",
              "       'male', 'phentermine', 'tirr', 'cf', 'wiil', 'construed', 'otcbb',\n",
              "       'atleast', 'materially', 'kin', '2005', 'vi', 'anticipates',\n",
              "       'erections', 'artprice', 'deciding', 'featured', 'prescriptions',\n",
              "       'sofftwaares', 'ali', 'ur', 'sir', 'discreet', 'gains', 'dose',\n",
              "       'cia', 'assurance', 'distributorjebel', 'nigeria', 'spur',\n",
              "       'serial', 'ambien', 'creative', 'align', 'stocks', 'aerofoam',\n",
              "       'der', 'penis', 'emerson', 'bingo', 'ffffffstrongfont', 'mai',\n",
              "       'style', 'anxiety', 'brbr', 'prozac', 'undervalued', 'epson',\n",
              "       'fontbr', 'notebook', 'levitra', 'es', 'iso', 'risks', 'alcohol',\n",
              "       'xm', 'erection', 'lasts', 'effects', 'vlagra', 'technoiogies',\n",
              "       '124', 'couid'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "top_x=200\n",
        "spam_ham_words[0:top_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2JAMfZJ3oKr"
      },
      "source": [
        "Note that words like `td`, `nbsp` and `br` are all HTML tags (for tables, spaces and newlines, respectively. This suggests that SPAM is more likely to have fancy HTML formatting than HAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXScvrE3oKr"
      },
      "source": [
        "Reverse the list, and now we have the \"hammiest\" words... (words most indicative of a legitimate email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "vnR6IcmF3oKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d6ee2f-7afe-48eb-c5db-80e9abc894c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['enron', 'ect', 'meter', 'hpl', 'daren', 'mmbtu', 'xls', 'pec',\n",
              "       'sitara', 'hou', 'volumes', 'ena', 'forwarded', 'melissa',\n",
              "       'tenaska', 'teco', 'nom', '2001', 'pat', 'aimee', 'actuals',\n",
              "       'noms', 'hsc', 'susan', 'cotten', 'chokshi', 'nomination', 'fyi',\n",
              "       'pipeline', 'wellhead', 'eastrans', 'clynes', 'hplc', '713',\n",
              "       'counterparty', 'pefs', 'bob', 'nominations', 'cec', 'gcs',\n",
              "       'lannou', 'txu', 'farmer', 'hplno', 'rita', 'weissman', 'cc',\n",
              "       'equistar', 'enronxgate', 'iferc', 'scheduled', 'spreadsheet',\n",
              "       'wynne', 'allocated', 'entex', 'path', 'buyback', 'fuels', 'hplo',\n",
              "       'lisa', 'scheduling', 'pops', 'anita', 'calpine', 'gco', 'darren',\n",
              "       'clem', 'steve', 'aep', 'katy', 'tu', 'flowed', 'follows',\n",
              "       'sherlyn', 'donna', 'lloyd', 'midcon', 'pm', 'redeliveries',\n",
              "       'jackie', 'gary', 'vance', 'papayoti', 'meters', 'cornhusker',\n",
              "       'luong', 'howard', 'pg', 'lsk', 'revision', 'julie', 'utilities',\n",
              "       '281', 'bryan', 'dfarmer', 'ees', 'reinhardt', 'hplnl', 'cleburne',\n",
              "       'valero', 'unify', 'outage', 'poorman', 'victor', 'methanol',\n",
              "       '6353', 'tap', 'baumbach', 'devon', 'lsp', 'lamphier', 'herod',\n",
              "       'liz', 'schumack', 'enserch', 'employee', '098', 'boas', 'megan',\n",
              "       'meyers', 'allocation', 'deliveries', 'easttexas', 'ami',\n",
              "       'enrononline', 'invoice', 'withers', 'taylor', 'robert', 'bellamy',\n",
              "       'fred', 'gpgfin', 'avila', 'pathed', 'duke', 'spoke', 'mccoy',\n",
              "       'cernosek', 'oasis', 'carlos', 'kevin', '1266', 'saturday', '853',\n",
              "       'riley', 'tejas', 'waha', 'katherine', 'kcs', 'graves',\n",
              "       'logistics', 'revised', 'paso', '345', 'eileen', 'hakemack', 'mm',\n",
              "       'ponton', 'cdnow', 'hesco', 'cp', 'reliantenergy', 'sandi', 'btu',\n",
              "       'mckay', 'gomes', 'chad', '0435', 'superty', 'lamadrid', '4179',\n",
              "       'tisdale', 'neon', 'lauri', 'interconnect', 'aepin', 'neuweiler',\n",
              "       'herrera', 'attached', 'panenergy', 'acton', 'tess', 'deal',\n",
              "       'rodriguez', 'mops', 'holmes', 'coastal', 'imbalance', 'stacey',\n",
              "       'availabilities', 'eol', 'pinion', 'heidi', 'camp', 'brenda',\n",
              "       'mary', 'origination', 'charlene', 'billed', 'lee'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "np.flip(spam_ham_words)[0:top_x]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given that a message is ham, how probable is it for the words to show up?\n",
        "print(\"Ham words log probabilities:\\n\", clf.feature_log_prob_[0])\n",
        "\n",
        "# Given that a message is spam, how probable is it for the words to show up?\n",
        "print(\"\\nSpam words log probabilities:\\n\", clf.feature_log_prob_[1])\n",
        "\n",
        "# Sort words by log probability for spam and ham classes\n",
        "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
        "spam_words = np.array(vectorizer.get_feature_names_out())[spam_args]\n",
        "spam_words = np.flip(spam_words)\n",
        "\n",
        "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
        "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
        "ham_words = np.flip(ham_words)\n",
        "\n",
        "# Print the top 10 spam and ham words\n",
        "print(\"\\nTop 10 spam words:\\n\", spam_words[:10])\n",
        "print(\"\\nTop 10 ham words:\\n\", ham_words[:10])\n",
        "\n",
        "# Calculate the log odds and sort words based on it\n",
        "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
        "spam_ham_args = np.argsort(log_odds)\n",
        "spam_ham_words = np.array(vectorizer.get_feature_names_out())[spam_ham_args]\n",
        "spam_ham_words = np.flip(spam_ham_words)\n",
        "\n",
        "top_x = 200\n",
        "print(\"\\nTop {} spammiest words:\\n\".format(top_x), spam_ham_words[:top_x])\n",
        "\n",
        "# Reverse the list to get the \"hammiest\" words\n",
        "print(\"\\nTop {} hammiest words:\\n\".format(top_x), np.flip(spam_ham_words)[:top_x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2brZ3ArX1rM",
        "outputId": "a5f66476-7084-435b-d737-1143cb09b235"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ham words log probabilities:\n",
            " [ -5.75023447  -5.71398455 -11.28296498 ... -13.07472445 -13.07472445\n",
            " -13.07472445]\n",
            "\n",
            "Spam words log probabilities:\n",
            " [ -6.23363088  -7.07284789  -9.8691907  ... -11.74099287 -11.74099287\n",
            " -11.74099287]\n",
            "\n",
            "Top 10 spam words:\n",
            " ['the' 'to' 'and' 'of' 'in' 'you' 'for' 'this' 'is' 'your']\n",
            "\n",
            "Top 10 ham words:\n",
            " ['the' 'to' 'ect' 'for' 'and' 'hou' 'enron' 'subject' 'on' 'of']\n",
            "\n",
            "Top 200 spammiest words:\n",
            " ['td' 'nbsp' 'pills' 'width' 'computron' 'br' 'font' 'href' 'viagra'\n",
            " 'height' 'xp' 'src' '2004' 'cialis' 'soft' 'meds' 'paliourg' 'php' 'voip'\n",
            " 'drugs' 'oo' 'valign' 'bgcolor' 'biz' 'hotlist' 'moopid' 'div'\n",
            " 'photoshop' 'mx' 'img' 'knle' 'pharmacy' 'gr' 'intel' 'corel'\n",
            " 'prescription' 'iit' 'demokritos' 'rolex' 'xanax' 'macromedia' 'dealer'\n",
            " 'uncertainties' 'valium' 'htmlimg' 'darial' '000000' '0310041' 'lots'\n",
            " 'projections' 'jebel' 'adobe' 'rnd' 'color' 'alt' '161' 'colspan' 'pain'\n",
            " 'readers' 'rx' 'canon' 'export' 'draw' 'fontfont' 'gra' 'speculative'\n",
            " '1226030' 'gold' 'pro' 'logos' 'wi' 'toshiba' 'china' '1933' 'spam'\n",
            " 'vicodin' 'itoy' 'viewsonic' 'ooking' '1618' 'cellpadding' 'weight'\n",
            " 'hewlett' '4176' 'pill' 'robotics' 'soma' 'resellers' '8834464' '8834454'\n",
            " 'apc' 'intellinet' 'aopen' 'iomega' 'enquiries' 'customerservice'\n",
            " 'targus' 'packard' 'tr' 'uae' 'dealers' 'spain' 'nomad' '1934' 'drug'\n",
            " 'muscle' 'abdv' 'zonedubai' 'eogi' 'aeor' 'doctors' 'inherent' 'wysak'\n",
            " 'emirates' 'cheap' 'health' 'border' 'illustrator' 'hottlist' 'oem'\n",
            " 'apple' 'ffffff' 'ce' 'verdana' 'sex' 'gif' 'resuits' 'graphics' 'mining'\n",
            " 'studio' 'differ' 'materia' 'predictions' 'arial' 'waste' 'cellspacing'\n",
            " 'yap' 'male' 'phentermine' 'tirr' 'cf' 'wiil' 'construed' 'otcbb'\n",
            " 'atleast' 'materially' 'kin' '2005' 'vi' 'anticipates' 'erections'\n",
            " 'artprice' 'deciding' 'featured' 'prescriptions' 'sofftwaares' 'ali' 'ur'\n",
            " 'sir' 'discreet' 'gains' 'dose' 'cia' 'assurance' 'distributorjebel'\n",
            " 'nigeria' 'spur' 'serial' 'ambien' 'creative' 'align' 'stocks' 'aerofoam'\n",
            " 'der' 'penis' 'emerson' 'bingo' 'ffffffstrongfont' 'mai' 'style'\n",
            " 'anxiety' 'brbr' 'prozac' 'undervalued' 'epson' 'fontbr' 'notebook'\n",
            " 'levitra' 'es' 'iso' 'risks' 'alcohol' 'xm' 'erection' 'lasts' 'effects'\n",
            " 'vlagra' 'technoiogies' '124' 'couid']\n",
            "\n",
            "Top 200 hammiest words:\n",
            " ['enron' 'ect' 'meter' 'hpl' 'daren' 'mmbtu' 'xls' 'pec' 'sitara' 'hou'\n",
            " 'volumes' 'ena' 'forwarded' 'melissa' 'tenaska' 'teco' 'nom' '2001' 'pat'\n",
            " 'aimee' 'actuals' 'noms' 'hsc' 'susan' 'cotten' 'chokshi' 'nomination'\n",
            " 'fyi' 'pipeline' 'wellhead' 'eastrans' 'clynes' 'hplc' '713'\n",
            " 'counterparty' 'pefs' 'bob' 'nominations' 'cec' 'gcs' 'lannou' 'txu'\n",
            " 'farmer' 'hplno' 'rita' 'weissman' 'cc' 'equistar' 'enronxgate' 'iferc'\n",
            " 'scheduled' 'spreadsheet' 'wynne' 'allocated' 'entex' 'path' 'buyback'\n",
            " 'fuels' 'hplo' 'lisa' 'scheduling' 'pops' 'anita' 'calpine' 'gco'\n",
            " 'darren' 'clem' 'steve' 'aep' 'katy' 'tu' 'flowed' 'follows' 'sherlyn'\n",
            " 'donna' 'lloyd' 'midcon' 'pm' 'redeliveries' 'jackie' 'gary' 'vance'\n",
            " 'papayoti' 'meters' 'cornhusker' 'luong' 'howard' 'pg' 'lsk' 'revision'\n",
            " 'julie' 'utilities' '281' 'bryan' 'dfarmer' 'ees' 'reinhardt' 'hplnl'\n",
            " 'cleburne' 'valero' 'unify' 'outage' 'poorman' 'victor' 'methanol' '6353'\n",
            " 'tap' 'baumbach' 'devon' 'lsp' 'lamphier' 'herod' 'liz' 'schumack'\n",
            " 'enserch' 'employee' '098' 'boas' 'megan' 'meyers' 'allocation'\n",
            " 'deliveries' 'easttexas' 'ami' 'enrononline' 'invoice' 'withers' 'taylor'\n",
            " 'robert' 'bellamy' 'fred' 'gpgfin' 'avila' 'pathed' 'duke' 'spoke'\n",
            " 'mccoy' 'cernosek' 'oasis' 'carlos' 'kevin' '1266' 'saturday' '853'\n",
            " 'riley' 'tejas' 'waha' 'katherine' 'kcs' 'graves' 'logistics' 'revised'\n",
            " 'paso' '345' 'eileen' 'hakemack' 'mm' 'ponton' 'cdnow' 'hesco' 'cp'\n",
            " 'reliantenergy' 'sandi' 'btu' 'mckay' 'gomes' 'chad' '0435' 'superty'\n",
            " 'lamadrid' '4179' 'tisdale' 'neon' 'lauri' 'interconnect' 'aepin'\n",
            " 'neuweiler' 'herrera' 'attached' 'panenergy' 'acton' 'tess' 'deal'\n",
            " 'rodriguez' 'mops' 'holmes' 'coastal' 'imbalance' 'stacey'\n",
            " 'availabilities' 'eol' 'pinion' 'heidi' 'camp' 'brenda' 'mary'\n",
            " 'origination' 'charlene' 'billed' 'lee']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# # Combine spam and ham words\n",
        "# top_words = np.concatenate((spam_words[:200], ham_words[:200]))\n",
        "\n",
        "# # Get the feature vectors for the top words\n",
        "# top_word_vectors = np.array([vectorizer.transform([word]).toarray()[0] for word in top_words])\n",
        "\n",
        "# # Apply t-SNE to project the word vectors into 2D\n",
        "# tsne = TSNE(n_components=2, random_state=random_seed)\n",
        "# word_vectors_2d = tsne.fit_transform(top_word_vectors)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Create a scatter plot of the t-SNE projected word vectors\n",
        "# plt.figure(figsize=(12, 12))\n",
        "\n",
        "# # Plot spam words in red\n",
        "# plt.scatter(word_vectors_2d[:200, 0], word_vectors_2d[:200, 1], c='red', label='Spam')\n",
        "\n",
        "# # Plot ham words in blue\n",
        "# plt.scatter(word_vectors_2d[200:, 0], word_vectors_2d[200:, 1], c='blue', label='Ham')\n",
        "\n",
        "# # Set plot title and axis labels\n",
        "# plt.title('t-SNE Clustering of Top Spam and Ham Words')\n",
        "# plt.xlabel('t-SNE Dimension 1')\n",
        "# plt.ylabel('t-SNE Dimension 2')\n",
        "\n",
        "# # Add a legend\n",
        "# plt.legend()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "g_wKWgzsdIZ4"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OJFrq33oKr"
      },
      "source": [
        "**Look at the words that best distinguish SPAM and HAM:**\n",
        "1. How many of your words showed up in the SPAM and HAM top 200 predictive words?\n",
        "2. Are they what you would have expected?\n",
        "3. Based on this, what can you say about the differences between how people make prediction and ML algorithms make predictions?\n",
        "4. Does this make you more confident or less confident in ML predictions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgog1Tcg3oKr"
      },
      "source": [
        "**Discuss here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRBVr7773oKr"
      },
      "source": [
        "# 2) Bayesian Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aycL1C93oKr"
      },
      "source": [
        "In this problem, we'll be using the `ASIA` dataset, which showcases the reltionships between travel, smoking, etc. and the probabilty of having various conditions. We will be using the [pomegranate](https://pomegranate.readthedocs.io/en/latest/) library to handle the heavy lifting for Bayes Nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4t7iEI3oKr"
      },
      "source": [
        "![](./asia_data.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nDjVtV3oKr"
      },
      "source": [
        "## 2.1) Exploring Bayes Nets (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwcDLa-3oKr"
      },
      "source": [
        "First go here: https://www.bayesserver.com/examples/networks/asia\n",
        "\n",
        "Try checking different boxes and seeing how the model updates. When you check a box, you're \"given\" a specific value for that node. For example, checking \"True\" for \"Visit to Asia\" means the patient has visited Asia, but we don't know the other probabilities yet. The new probabilities are \"given\" that you've visited Asia.\n",
        "\n",
        "Now answer the following questions. For each one, first make a **prediction** about how the model will change, and the try it to see if you're right. Write down your prediction and then the actual answer. If your prediction differs than the actual answer, try and discuss why.\n",
        "\n",
        "**After each question, uncheck all boxes.**\n",
        "\n",
        "1. If you set the value of Visit to Asia, which nodes will update?\n",
        "2. If you set the value of XRay Result, which nodes with update?\n",
        "3. First set the value for Has Tuberculosis. If you then set the value for Visit to Asia, which nodes will update?\n",
        "4. First set the value for Tuberculosis or Cancer. If you then set the value for Dyspnea, which nodes will update?\n",
        "5. If you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "6. First set the value for Tuberculosis or Cancer. Now if you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "\n",
        "If you wish to understand more about conditional independence and D-Separation, go here:  https://www.youtube.com/watch?v=_R_RYn5KelA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh6nOQi03oKs"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrtNDwaW3oKs"
      },
      "source": [
        "## 2.2) Building the Bayes Net (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlDKW5G3oKs"
      },
      "source": [
        "Just like in class, we can define the initial structure and conditional probability tables for the Bayes Net using our expert knowledge of the scenario (in this case, given to use by experts).\n",
        "\n",
        "For example, the first table gives the probability of having TB give that you have (or have not) visited Asia.\n",
        "\n",
        "| Asia | HasTB | P(HasTB\\|Asia) |\n",
        "| ---- | ----- | ------------- |\n",
        "| T | T | 0.05 |\n",
        "| T | F | 0.95 |\n",
        "| F | T | 0.01 |\n",
        "| F | F | 0.99 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToHUzunj3oKs"
      },
      "outputs": [],
      "source": [
        "from pomegranate import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTqq_CrP3oKs"
      },
      "outputs": [],
      "source": [
        "# First, we define our top level nodes with their base probabilities.\n",
        "\n",
        "visit_to_asia = DiscreteDistribution({'T':0.01, 'F':0.99})\n",
        "smoke = DiscreteDistribution({'T':0.5, 'F':0.5})\n",
        "\n",
        "# Now, we have to fill in all of the conditional probability tables for the other nodes\n",
        "\n",
        "has_tb = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Asia? #HasTB #Probability\n",
        "        [\"T\",\"T\",0.05],\n",
        "        [\"T\",\"F\",0.95],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99],\n",
        "    ], [visit_to_asia])\n",
        "\n",
        "\n",
        "has_lung_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke? \n",
        "        [\"T\",\"T\",0.1],\n",
        "        [\"T\",\"F\",0.9],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99]\n",
        "    ], [smoke])\n",
        "\n",
        "\n",
        "has_bc = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke?\n",
        "        [\"T\",\"T\",0.6],\n",
        "        [\"T\",\"F\",0.4],\n",
        "        \n",
        "        [\"F\",\"T\",0.3],\n",
        "        [\"F\",\"F\",0.7]\n",
        "    ], [smoke])\n",
        "\n",
        "tb_or_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Lung? TB? \n",
        "        [\"T\",\"T\",\"T\",1],\n",
        "        [\"T\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",1],\n",
        "        [\"T\",\"F\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",1],\n",
        "        [\"F\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0],\n",
        "        [\"F\",\"F\",\"F\",1]\n",
        "    ], [has_lung_cancer,has_tb])\n",
        "\n",
        "x_ray_abnormal = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #TB or Cancer?\n",
        "        [\"T\",\"T\",0.98],\n",
        "        [\"T\",\"F\",0.02],\n",
        "        \n",
        "        [\"F\",\"T\",0.05],\n",
        "        [\"F\",\"F\",0.95]\n",
        "    ], [tb_or_cancer])\n",
        "\n",
        "dyspnea = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #BC\n",
        "        [\"T\",\"T\",\"T\",0.9],\n",
        "        [\"T\",\"T\",\"F\",0.1],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",0.8],\n",
        "        [\"T\",\"F\",\"F\",0.2],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",0.7],\n",
        "        [\"F\",\"T\",\"F\",0.3],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0.1],\n",
        "        [\"F\",\"F\",\"F\",0.9]\n",
        "    ], [has_bc, tb_or_cancer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGmUvsL53oKs"
      },
      "outputs": [],
      "source": [
        "# Next we have to create all the nodes\n",
        "asia_node = Node(visit_to_asia, name=\"asia\")\n",
        "tb_node = Node(has_tb, name=\"tb\")\n",
        "smoke_node = Node(smoke, name=\"smoke\")\n",
        "lung_node = Node(has_lung_cancer, name=\"lung\")\n",
        "bronc_node = Node(has_bc, name=\"bc\")\n",
        "either_node = Node(tb_or_cancer, name=\"either\")\n",
        "xray_node = Node(x_ray_abnormal,name=\"xray\")\n",
        "dysp_node = Node(dyspnea, name=\"dysp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S40oXQvn3oKs"
      },
      "outputs": [],
      "source": [
        "# Now we init the model\n",
        "model = BayesianNetwork(\"ASIA\")\n",
        "model.add_states(asia_node,\n",
        "                 tb_node,\n",
        "                 smoke_node,\n",
        "                 lung_node,\n",
        "                 bronc_node,\n",
        "                 either_node,\n",
        "                 xray_node,\n",
        "                 dysp_node)\n",
        "\n",
        "# Add all of the correct edges \n",
        "model.add_edge(asia_node, tb_node)\n",
        "\n",
        "model.add_edge(smoke_node, bronc_node)\n",
        "model.add_edge(smoke_node, lung_node)\n",
        "\n",
        "model.add_edge(tb_node,either_node)\n",
        "model.add_edge(lung_node,either_node)\n",
        "\n",
        "model.add_edge(either_node, xray_node)\n",
        "model.add_edge(either_node, dysp_node)\n",
        "\n",
        "model.add_edge(bronc_node, dysp_node)\n",
        "\n",
        "# And then commit our changes\n",
        "model.bake()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoO5th503oKs"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the model structure\n",
        "def print_model_structure(model, features):\n",
        "    for i in range(len(features)):\n",
        "        parents = [features[pi] for pi in model.structure[i]]\n",
        "        print(f'Node \"{features[i]}\" has parents: {parents}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OSXGErC3oKs"
      },
      "outputs": [],
      "source": [
        "# We'll keep our features in this order for consistency\n",
        "features = [\n",
        "    \"Visit to Asia\",\n",
        "    \"Has TB\",\n",
        "    \"Smoker\",\n",
        "    \"Has Lung Cancer\",\n",
        "    \"Has Bronchitis\",\n",
        "    \"TB or Cancer\",\n",
        "    \"XRay Abnormal\",\n",
        "    \"Dyspnea\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ5vxsyM3oKs"
      },
      "outputs": [],
      "source": [
        "# Let's make sure the structure of our newly created model is correct\n",
        "print_model_structure(model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPamBaC3oKs"
      },
      "source": [
        "## 2.3) Predictions (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdmmYfwg3oKs"
      },
      "source": [
        "`predict` allows us to do inference based off of the data. It chooses the values that are the most likely.\n",
        "\n",
        "For example, let's say we have a patient who has an abnormal X-ray, but is not a smoker and hasn't visited Asia. We can then infer the most likely values for all of the other variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Old8Bu-33oKt"
      },
      "outputs": [],
      "source": [
        "model.predict([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJwMizsQ3oKt"
      },
      "outputs": [],
      "source": [
        "# Now let's say they *were* a smoker. See how it changes?\n",
        "model.predict([\n",
        "    [\"F\",None,\"T\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVPgog63oKt"
      },
      "source": [
        "We can get a little more detail and check out the actual probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXCd4FWR3oKt"
      },
      "outputs": [],
      "source": [
        "def pretty_results(results):\n",
        "    for i,dist in enumerate(results):\n",
        "        print(features[i])\n",
        "        if isinstance(dist,str):\n",
        "            print(dist)\n",
        "        else:\n",
        "            print(dist.parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo8xV7iX3oKt"
      },
      "outputs": [],
      "source": [
        "res = model.predict_proba([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])\n",
        "pretty_results(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyXMsO-3oKt"
      },
      "source": [
        "Use the Bayes net to calcualte the following probabilities:\n",
        "\n",
        "1. $P(xray=true | TBorCancer=true)$\n",
        "2. $P(xray=true | TBorCancer=true, TB=true)$\n",
        "3. $P(TB=true)$\n",
        "4. $P(TB=true | smoke=false)$\n",
        "5. $P(TB=true | smoke=false, TBorCancer=true)$\n",
        "\n",
        "What values are equivalent? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA2KHRPl3oKt"
      },
      "source": [
        "**Write your answers here:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5.\n",
        "\n",
        "What values are equivalent? Why?\n",
        "\n",
        "And write code below to help you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBK7Kl-Y3oKt"
      },
      "outputs": [],
      "source": [
        "# As a reminder, here are the indices of the features\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXtRvJ363oKt"
      },
      "outputs": [],
      "source": [
        "# You can modify this code to help you\n",
        "pretty_results(model.predict_proba([\n",
        "    [None,None,'F',None,None,'T',None,None]\n",
        "])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvFDNx3H3oKt"
      },
      "source": [
        "## 2.4) Evaluating Bayes Nets (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dOhNguE3oKt"
      },
      "source": [
        "Let's see how well this net is on inferencing from data. We're going to remove the Bronchitis column from this dataset, and see if our net can predict what the missing value should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzPG51iF3oKt"
      },
      "outputs": [],
      "source": [
        "# Some data we will use to generate our probabilities\n",
        "asia_data = pd.read_csv(\"Asia10k.csv\")\n",
        "asia_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_23SlnAX3oKu"
      },
      "outputs": [],
      "source": [
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTJ5Cqjn3oKu"
      },
      "outputs": [],
      "source": [
        "# Let's make sure we're consistant with our labels\n",
        "asia_data = asia_data.replace(\"no\", \"F\").replace(\"yes\", \"T\")\n",
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y4ltbBJ3oKu"
      },
      "outputs": [],
      "source": [
        "values = asia_data.values.copy()\n",
        "indices = np.random.choice(asia_data.index, 1000)\n",
        "values = values[indices]\n",
        "values[:,4] = None\n",
        "values[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DJTLoG03oKu"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMAVAWm3oKu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDkwBqa3oKu"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(asia_data.values[indices,4],np.array(predictions)[:,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4hBVGX3oKu"
      },
      "outputs": [],
      "source": [
        "print(classification_report(asia_data.values[indices,4],np.array(predictions)[:,4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHJubZb3oKu"
      },
      "source": [
        "## 2.5) Fitting a Bayes Net to Data (Group)\n",
        "\n",
        "In many applications, you may have a general idea of the structure of the Bayes Net, but do not have a list of probabilities. Luckily, given some data, we can fill out the probabilities in a given net. **Note:** You may only get similar results to the previous method, since it turns out this data was *simulated* from the given conditional probabilities. So, one would expect that the model would learn parameters like the ones we've given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83gWfLiD3oKu"
      },
      "outputs": [],
      "source": [
        "fitted_model = model.fit(asia_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_0JlNi3oKu"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the probability distributions\n",
        "def print_distributions(model):\n",
        "    for i, state in enumerate(fitted_model.states):\n",
        "        print(features[i])\n",
        "        states = state.distribution.parameters[0]\n",
        "        if len(states)>1:\n",
        "            for state in states:\n",
        "                print(state)\n",
        "        #print(state.distribution.parameters[0])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_8lKDiQ3oKu"
      },
      "outputs": [],
      "source": [
        "print_distributions(fitted_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB0ccZug3oKu"
      },
      "source": [
        "**Take a look at the learned probability distributions. Are they similar to \"expert\" ones given in the previous problem?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8751vMuM3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlViQF7V3oKv"
      },
      "source": [
        "Now, perform the same evaluation that you did in the previous problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNI7EMf53oKv"
      },
      "outputs": [],
      "source": [
        "# Remove Some other column other than Bronchitis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOdZ0cKE3oKv"
      },
      "outputs": [],
      "source": [
        "# Make the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdqbnXB23oKv"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0lXJgal3oKv"
      },
      "outputs": [],
      "source": [
        "# Classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPgf3l13oKv"
      },
      "source": [
        "How well did it perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQWm4yT3oKv"
      },
      "source": [
        "## 2.6) Learning Structure from Data (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIlFb6YZ3oKv"
      },
      "source": [
        "Now, the most interesting problem is when we only have data, but we don't know the structure of the data (however, we still have a reason to believe that *it can be reperesented as Bayes Net*). Luckily, pomegranate has the ability to solve this problem as well. Given a dataset, we can use `from_samples` to build a Bayes net, structure and all, from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQw2ci3r3oKv"
      },
      "outputs": [],
      "source": [
        "learned_model = BayesianNetwork.from_samples(asia_data, algorithm='exact')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_afVm0o43oKv"
      },
      "outputs": [],
      "source": [
        "print_model_structure(learned_model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyEpFS_3oKv"
      },
      "source": [
        "**Compare the model structure from experts vs learned from the data:**\n",
        "\n",
        "1. Draw out both models (the one you made earlier and the one learned) on a piece of paper.\n",
        "2. What are the differences you observe?\n",
        "3. Why might a model learned from the data have a different structure? Are some influences (edges) in the model more or less important than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnzxe7sv3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7GIWIFG3oKw"
      },
      "source": [
        "## 2.7) BYOB: Build Your Own Bayes Net (Group, if time permits)\n",
        "If you find yourself with some extra time after this portion, consider **building a Bayes net that represents something in your daily life**. It could be the effect of traffic on a morning commute, deciding what to do for dinner, etc. It can be very small, only around 3-5 nodes probably (conditional probability tables are a pain!). Then play around with predictions and probabilities to see how various factors impact your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzIWCRVT3oKw"
      },
      "outputs": [],
      "source": [
        "# Start building!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}