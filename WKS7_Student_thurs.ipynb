{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/WKS7_Student_thurs_nau.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohSsvxj3oKY"
      },
      "source": [
        "# Workshop 7: Bayes Rule Rules\n",
        "\n",
        "In this workshop, we'll be looking at how to use Naive Bayes and Bayes Nets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# This workshop was the first time I had to find a workaround because I refuse \n",
        "# to install conda on my personal machine. The package that is specified, Pomegranate, \n",
        "# will not run on Google Colab. I cannot find any solution on StackOverflow or in the \n",
        "# docs. I will update this notebook when I have a solution that is not, preferably, install conda."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfpOZpT36JR",
        "outputId": "5473ef34-06c0-44c7-f15b-c3188545e2b6"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install scipy \n",
        "!pip install scikit-learn \n",
        "!pip install torch \n",
        "!pip install apricot-select \n",
        "!pip install networkx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6hOorp6oedZ",
        "outputId": "7e2eff3f-f7a4-4bec-c0e3-6be3a99d4a44"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: apricot-select in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from apricot-select) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from apricot-select) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.10/dist-packages (from apricot-select) (0.56.4)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from apricot-select) (4.65.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from apricot-select) (1.3.7)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->apricot-select) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->apricot-select) (67.7.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Rdl7Os3oKZ"
      },
      "source": [
        "# 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "uwRsl8_y3oKZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "# set a seed for reproducibility\n",
        "random_seed = 25\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDRXoMEM3oKa"
      },
      "source": [
        "# 1) Naive Bayes Spam Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AekfdRJp3oKa"
      },
      "source": [
        "One historical use of Naive Bayes is to try and detect [spam emails](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering). \n",
        "\n",
        "In this exercise, you will be using dataset that of emails from the [Enron Corporation](https://en.wikipedia.org/wiki/Enron_Corpus), an accounting firm that [went bankrupt in 2001 due to an accounting scandal](https://en.wikipedia.org/wiki/Enron_scandal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUnQ3Kn3oKa"
      },
      "source": [
        "## 1.1) Exploring the Data (Follow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "J0b-zYmP3oKa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/AI ACADEMY/2 - Data Mining/7- Week 7/WKS7_Student/enron_emails.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "X-yYqAY23oKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fce320d-d393-46ed-fef6-5961df020100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham     3672\n",
            "spam    1499\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Ham is a legitimate email, while spam is unwanted\n",
        "# Let's look at our distribution of spam and ham emails\n",
        "email_counts = df.label.value_counts()\n",
        "print(email_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunYeUfS3oKb"
      },
      "source": [
        "Keeping with the theme of meat products, some researchers call emails that are *not spam*, **ham**. \n",
        "\n",
        "To sum up: a **ham** email is a legitimate email, while a **spam** email is unwanted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "usZGaAqi3oKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efc9c77-4554-430b-9b63-0e2d82b8de6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     3672\n",
              "spam    1499\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "# Let's look at our distriubtion of spam and ham emails\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfmrjIe3oKb"
      },
      "source": [
        "Let's explore some of the ham emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "f-bzWi7k3oKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a655fb10-5f50-4d2e-b496-de8af69f7ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: ehronline web address change\n",
            "this message is intended for ehronline users only .\n",
            "due to a recent change to ehronline , the url ( aka \" web address \" ) for accessing ehronline needs to be changed on your computer . the change involves adding the letter \" s \" to the \" http \" reference in the url . the url for accessing ehronline should be : https : / / ehronline . enron . com .\n",
            "this change should be made by those who have added the url as a favorite on the browser .\n"
          ]
        }
      ],
      "source": [
        "print(df[df[\"label\"]==\"ham\"].text.iloc[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZPf3_D3oKc"
      },
      "source": [
        "And now the spam emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "HcYfLy5f3oKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cd9f421-d897-4962-d26b-adaeb8658d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: back\n",
            "emile (\n",
            "the cablefilterz will allow you to receive\n",
            "all the channels that you order with your remote control ,\n",
            "payperviews , axxxmovies , sport events , special - events !\n",
            "http : / / www . 8006 hosting . com / cable /\n",
            "avocation , despoil .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df[df[\"label\"]==\"spam\"].text.iloc[18])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4358Uhc3oKc"
      },
      "source": [
        "Try exploring different emails by changing the index in the lines above. **What common traits do you notice accross the ham emails? The spam emails?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Ckb1nD3oKc"
      },
      "source": [
        "## 1.2) Bag of Words (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ52GomW3oKc"
      },
      "source": [
        "Last week we used tf-idf to represent words as feature vectors. However, sometimes simpler methods work just as well (if not better). For this, we'll be using the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation of a piece of text, which is much more interpretable than tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ZerWdYep3oKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75d7296-4e18-4f3c-dad3-46e61f7b24f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 9)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvnXPhR3oKd"
      },
      "source": [
        "The CountVectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how many times word `m` appears in document `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5lUP3XGj3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73a94a7-7531-45ed-d3ff-e26f86156b9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AMYkf9z53oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4190aae0-9959-4427-be17-e21994e3e442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F87S6aFI3oKd"
      },
      "source": [
        "A more interpretable view..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iJtkLpYz3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc6f9c6-da69-474c-fa17-bafebb8e0b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first document.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 1,\n",
              " 'first': 1,\n",
              " 'is': 1,\n",
              " 'one': 0,\n",
              " 'second': 0,\n",
              " 'the': 1,\n",
              " 'third': 0,\n",
              " 'this': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "print(corpus[0])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XtSNslSR3oKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae0db47-8966-46ac-cba9-b9a13555ec37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This document is the second document.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'document': 2,\n",
              " 'first': 0,\n",
              " 'is': 1,\n",
              " 'one': 0,\n",
              " 'second': 1,\n",
              " 'the': 1,\n",
              " 'third': 0,\n",
              " 'this': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "print(corpus[1])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1PaPk03oKe"
      },
      "source": [
        "Now, if you want to vectorize new data (e.g. test data), then you use the `.transform` function. If the vectorizer encounters a word it hasn't seen before, it will simply ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bKEga4MD3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e3a7ebe-2ee0-4d08-e0ae-7d6b5e9f4c3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vectorizer.transform([\"This is the coolest document\"]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp85K6mp3oKe"
      },
      "source": [
        "# 1.3) Building and Running the Model (Group)\n",
        "\n",
        "Now that you have all the required tools, build a **Naive Bayes Classifier** and evaluate it on a train and test set. In this instance, [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier, which is most useful for discrete features that use frequency counts (e.g. a bag of words vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-y-VAl3y3oKe"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e496K1us3oKe"
      },
      "outputs": [],
      "source": [
        "# Create training and test splits - 20% split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=random_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QtaDlE1I3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize on your training data using BoW\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xxhRt-pN3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "e0759547-6be1-4ae8-b0fa-1158864e095f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Fit the classifier below\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mMuvREFq3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize your test data using transform and then predict the test data\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "y_pred = clf.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "esP-NBkG3oKe"
      },
      "outputs": [],
      "source": [
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "37vUPoIV3oKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e21367e-714a-4c8e-e41b-c8537d4f2ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[728   6]\n",
            " [ 14 287]]\n"
          ]
        }
      ],
      "source": [
        "# Print a confusion matrix using confusion_matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7VPtAci03oKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f91cfd7-f772-49b1-cd18-50abed7e3123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      0.99      0.99       734\n",
            "        spam       0.98      0.95      0.97       301\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.97      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print a classification report using classification_report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code from Lori K:\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=random_seed)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(train.text)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X,train.label_num)\n",
        "test_vecs = vectorizer.transform(test.text)\n",
        "predictions = clf.predict(test_vecs)\n",
        "confusion_matrix(test.label_num,predictions)\n",
        "print(classification_report(test.label_num,predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DDWD1BhVyLJ",
        "outputId": "4494934e-cbfa-426f-aab0-c9225c177c11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       734\n",
            "           1       0.98      0.96      0.97       301\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.98      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ml1Cy23oKf"
      },
      "source": [
        "## 1.4) Exploring Important Words (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksJ4daT3oKf"
      },
      "source": [
        "Before you start, predict what words might be more predictive of SPAM or HAM. Make a list below of 5 words you think will be very _predictive_ of an email being SPAM, and 5 words that are predictive of being HAM. Remember this is an office email database from Enron in the 1990s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_S81Onz3oKf"
      },
      "source": [
        "Words the will predict SPAM (junk emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x\n",
        "\n",
        "Words the will predict HAM (real emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWA5Gk4B3oKf"
      },
      "source": [
        "**Technical Note: Log Probabilities**: \n",
        "\n",
        "When using probabilistic methods with large datasets, sometimes you get features with extremely small probabilities (e.g. $10^{-10}$). \n",
        "\n",
        "This becomes a problem, because computers aren't really good at doing operations with numbers at this scale. Therefore, in most systems, operations are done on the *log* of the probabilities. \n",
        "\n",
        "This makes calculations much more managable (e.g. $\\log(10^{-10})=-10$). As an added bonus, due to log rules ($log(ab)=log(a)+log(b)$), all multiplications turn into additions, which are easier for the computer.\n",
        "\n",
        "Some general rules of thumb: **the closer to zero a log prob is, the more probabable it is**, and **each time a log prob decreases by one, it's an order of magnitude less probable**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdS2iLHz3oKq"
      },
      "source": [
        "`feature_log_probs` gives us the log probabilities for each word. In notation, each of these are $P(word | class)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bU_k5dga3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629eb165-edc1-438d-cdce-db61dc5d7eae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -5.75023447,  -5.71398455, -11.28296498, ..., -13.07472445,\n",
              "       -13.07472445, -13.07472445])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Given that a message is ham, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "A-a_XpxM3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb10352-b0ba-4998-cdca-d33252114e1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -6.23363088,  -7.07284789,  -9.8691907 , ..., -11.74099287,\n",
              "       -11.74099287, -11.74099287])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Given that a message is SPAM, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub__FBPq3oKq"
      },
      "source": [
        "This code will sort all the words by log probability, so that all of the most probable words show up first..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ULJwZZO03oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72338786-0bec-44fe-8103-d445d2666c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the' 'to' 'and' ... 'brewer' 'breveffo' 'cima']\n",
            "['the' 'to' 'ect' ... 'luge' 'lugging' 'ikoogybhmxdc']\n"
          ]
        }
      ],
      "source": [
        "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
        "spam_words = np.array(vectorizer.get_feature_names_out())[spam_args]\n",
        "spam_words = np.flip(spam_words)\n",
        "print(spam_words)\n",
        "\n",
        "\n",
        "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
        "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
        "ham_words = np.flip(ham_words)\n",
        "print(ham_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZaUKsYJk3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8243f4fe-0974-46ed-932e-fce057ececcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['the', 'to', 'and', 'of', 'in', 'you', 'for', 'this', 'is', 'your'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "spam_words[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wIrC07iQ3oKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346c6a83-a97e-4268-c21d-83bb0f145bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['the', 'to', 'ect', 'for', 'and', 'hou', 'enron', 'subject', 'on',\n",
              "       'of'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "ham_words[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fweB7UQ3oKq"
      },
      "source": [
        "However, a more useful way to look at the data is to look at the *ratios* of the probabilities for a given word. For example, if we have the word \"free\":\n",
        "\n",
        "*If an email is spam, there is a 50% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|spam)=0.5$\n",
        "\n",
        "*If an email is ham, there is a 10% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|ham)=0.1$\n",
        "\n",
        "*The ratio*\n",
        "\n",
        "$P(free|spam)/P(free|ham)=5$\n",
        "\n",
        "This means that the word *free* is 5x as more likely to show up in spam messages compared to ham messages. So, we can use this to calculate and sort for words that are proportionally more present in spam emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2AFQz_xr3oKq"
      },
      "outputs": [],
      "source": [
        "# Since we're operating on logs, division turns into subtraction\n",
        "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
        "spam_ham_args = np.argsort(log_odds)\n",
        "spam_ham_words = np.array(vectorizer.get_feature_names_out())[spam_ham_args]\n",
        "spam_ham_words = np.flip(spam_ham_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13UPxtBZ3oKr"
      },
      "source": [
        "Here's some of the \"spammiest\" words..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mtmlqiOA3oKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03bb7bef-c916-4184-ec32-461998e235b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['td', 'nbsp', 'pills', 'width', 'computron', 'br', 'font', 'href',\n",
              "       'viagra', 'height', 'xp', 'src', '2004', 'cialis', 'soft', 'meds',\n",
              "       'paliourg', 'php', 'voip', 'drugs', 'oo', 'valign', 'bgcolor',\n",
              "       'biz', 'hotlist', 'moopid', 'div', 'photoshop', 'mx', 'img',\n",
              "       'knle', 'pharmacy', 'gr', 'intel', 'corel', 'prescription', 'iit',\n",
              "       'demokritos', 'rolex', 'xanax', 'macromedia', 'dealer',\n",
              "       'uncertainties', 'valium', 'htmlimg', 'darial', '000000',\n",
              "       '0310041', 'lots', 'projections', 'jebel', 'adobe', 'rnd', 'color',\n",
              "       'alt', '161', 'colspan', 'pain', 'readers', 'rx', 'canon',\n",
              "       'export', 'draw', 'fontfont', 'gra', 'speculative', '1226030',\n",
              "       'gold', 'pro', 'logos', 'wi', 'toshiba', 'china', '1933', 'spam',\n",
              "       'vicodin', 'itoy', 'viewsonic', 'ooking', '1618', 'cellpadding',\n",
              "       'weight', 'hewlett', '4176', 'pill', 'robotics', 'soma',\n",
              "       'resellers', '8834464', '8834454', 'apc', 'intellinet', 'aopen',\n",
              "       'iomega', 'enquiries', 'customerservice', 'targus', 'packard',\n",
              "       'tr', 'uae', 'dealers', 'spain', 'nomad', '1934', 'drug', 'muscle',\n",
              "       'abdv', 'zonedubai', 'eogi', 'aeor', 'doctors', 'inherent',\n",
              "       'wysak', 'emirates', 'cheap', 'health', 'border', 'illustrator',\n",
              "       'hottlist', 'oem', 'apple', 'ffffff', 'ce', 'verdana', 'sex',\n",
              "       'gif', 'resuits', 'graphics', 'mining', 'studio', 'differ',\n",
              "       'materia', 'predictions', 'arial', 'waste', 'cellspacing', 'yap',\n",
              "       'male', 'phentermine', 'tirr', 'cf', 'wiil', 'construed', 'otcbb',\n",
              "       'atleast', 'materially', 'kin', '2005', 'vi', 'anticipates',\n",
              "       'erections', 'artprice', 'deciding', 'featured', 'prescriptions',\n",
              "       'sofftwaares', 'ali', 'ur', 'sir', 'discreet', 'gains', 'dose',\n",
              "       'cia', 'assurance', 'distributorjebel', 'nigeria', 'spur',\n",
              "       'serial', 'ambien', 'creative', 'align', 'stocks', 'aerofoam',\n",
              "       'der', 'penis', 'emerson', 'bingo', 'ffffffstrongfont', 'mai',\n",
              "       'style', 'anxiety', 'brbr', 'prozac', 'undervalued', 'epson',\n",
              "       'fontbr', 'notebook', 'levitra', 'es', 'iso', 'risks', 'alcohol',\n",
              "       'xm', 'erection', 'lasts', 'effects', 'vlagra', 'technoiogies',\n",
              "       '124', 'couid'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "top_x=200\n",
        "spam_ham_words[0:top_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2JAMfZJ3oKr"
      },
      "source": [
        "Note that words like `td`, `nbsp` and `br` are all HTML tags (for tables, spaces and newlines, respectively. This suggests that SPAM is more likely to have fancy HTML formatting than HAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXScvrE3oKr"
      },
      "source": [
        "Reverse the list, and now we have the \"hammiest\" words... (words most indicative of a legitimate email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "vnR6IcmF3oKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbbeb49f-db65-4ac6-8ffa-2c12ffc41add"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['enron', 'ect', 'meter', 'hpl', 'daren', 'mmbtu', 'xls', 'pec',\n",
              "       'sitara', 'hou', 'volumes', 'ena', 'forwarded', 'melissa',\n",
              "       'tenaska', 'teco', 'nom', '2001', 'pat', 'aimee', 'actuals',\n",
              "       'noms', 'hsc', 'susan', 'cotten', 'chokshi', 'nomination', 'fyi',\n",
              "       'pipeline', 'wellhead', 'eastrans', 'clynes', 'hplc', '713',\n",
              "       'counterparty', 'pefs', 'bob', 'nominations', 'cec', 'gcs',\n",
              "       'lannou', 'txu', 'farmer', 'hplno', 'rita', 'weissman', 'cc',\n",
              "       'equistar', 'enronxgate', 'iferc', 'scheduled', 'spreadsheet',\n",
              "       'wynne', 'allocated', 'entex', 'path', 'buyback', 'fuels', 'hplo',\n",
              "       'lisa', 'scheduling', 'pops', 'anita', 'calpine', 'gco', 'darren',\n",
              "       'clem', 'steve', 'aep', 'katy', 'tu', 'flowed', 'follows',\n",
              "       'sherlyn', 'donna', 'lloyd', 'midcon', 'pm', 'redeliveries',\n",
              "       'jackie', 'gary', 'vance', 'papayoti', 'meters', 'cornhusker',\n",
              "       'luong', 'howard', 'pg', 'lsk', 'revision', 'julie', 'utilities',\n",
              "       '281', 'bryan', 'dfarmer', 'ees', 'reinhardt', 'hplnl', 'cleburne',\n",
              "       'valero', 'unify', 'outage', 'poorman', 'victor', 'methanol',\n",
              "       '6353', 'tap', 'baumbach', 'devon', 'lsp', 'lamphier', 'herod',\n",
              "       'liz', 'schumack', 'enserch', 'employee', '098', 'boas', 'megan',\n",
              "       'meyers', 'allocation', 'deliveries', 'easttexas', 'ami',\n",
              "       'enrononline', 'invoice', 'withers', 'taylor', 'robert', 'bellamy',\n",
              "       'fred', 'gpgfin', 'avila', 'pathed', 'duke', 'spoke', 'mccoy',\n",
              "       'cernosek', 'oasis', 'carlos', 'kevin', '1266', 'saturday', '853',\n",
              "       'riley', 'tejas', 'waha', 'katherine', 'kcs', 'graves',\n",
              "       'logistics', 'revised', 'paso', '345', 'eileen', 'hakemack', 'mm',\n",
              "       'ponton', 'cdnow', 'hesco', 'cp', 'reliantenergy', 'sandi', 'btu',\n",
              "       'mckay', 'gomes', 'chad', '0435', 'superty', 'lamadrid', '4179',\n",
              "       'tisdale', 'neon', 'lauri', 'interconnect', 'aepin', 'neuweiler',\n",
              "       'herrera', 'attached', 'panenergy', 'acton', 'tess', 'deal',\n",
              "       'rodriguez', 'mops', 'holmes', 'coastal', 'imbalance', 'stacey',\n",
              "       'availabilities', 'eol', 'pinion', 'heidi', 'camp', 'brenda',\n",
              "       'mary', 'origination', 'charlene', 'billed', 'lee'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "np.flip(spam_ham_words)[0:top_x]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Given that a message is ham, how probable is it for the words to show up?\n",
        "print(\"Ham words log probabilities:\\n\", clf.feature_log_prob_[0])\n",
        "\n",
        "# Given that a message is spam, how probable is it for the words to show up?\n",
        "print(\"\\nSpam words log probabilities:\\n\", clf.feature_log_prob_[1])\n",
        "\n",
        "# Sort words by log probability for spam and ham classes\n",
        "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
        "spam_words = np.array(vectorizer.get_feature_names_out())[spam_args]\n",
        "spam_words = np.flip(spam_words)\n",
        "\n",
        "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
        "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
        "ham_words = np.flip(ham_words)\n",
        "\n",
        "# Print the top 10 spam and ham words\n",
        "print(\"\\nTop 10 spam words:\\n\", spam_words[:10])\n",
        "print(\"\\nTop 10 ham words:\\n\", ham_words[:10])\n",
        "\n",
        "# Calculate the log odds and sort words based on it\n",
        "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
        "spam_ham_args = np.argsort(log_odds)\n",
        "spam_ham_words = np.array(vectorizer.get_feature_names_out())[spam_ham_args]\n",
        "spam_ham_words = np.flip(spam_ham_words)\n",
        "\n",
        "top_x = 200\n",
        "print(\"\\nTop {} spammiest words:\\n\".format(top_x), spam_ham_words[:top_x])\n",
        "\n",
        "# Reverse the list to get the \"hammiest\" words\n",
        "print(\"\\nTop {} hammiest words:\\n\".format(top_x), np.flip(spam_ham_words)[:top_x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2brZ3ArX1rM",
        "outputId": "f84b3ee8-1d36-4a29-8d76-bb15f2758c03"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ham words log probabilities:\n",
            " [ -5.75023447  -5.71398455 -11.28296498 ... -13.07472445 -13.07472445\n",
            " -13.07472445]\n",
            "\n",
            "Spam words log probabilities:\n",
            " [ -6.23363088  -7.07284789  -9.8691907  ... -11.74099287 -11.74099287\n",
            " -11.74099287]\n",
            "\n",
            "Top 10 spam words:\n",
            " ['the' 'to' 'and' 'of' 'in' 'you' 'for' 'this' 'is' 'your']\n",
            "\n",
            "Top 10 ham words:\n",
            " ['the' 'to' 'ect' 'for' 'and' 'hou' 'enron' 'subject' 'on' 'of']\n",
            "\n",
            "Top 200 spammiest words:\n",
            " ['td' 'nbsp' 'pills' 'width' 'computron' 'br' 'font' 'href' 'viagra'\n",
            " 'height' 'xp' 'src' '2004' 'cialis' 'soft' 'meds' 'paliourg' 'php' 'voip'\n",
            " 'drugs' 'oo' 'valign' 'bgcolor' 'biz' 'hotlist' 'moopid' 'div'\n",
            " 'photoshop' 'mx' 'img' 'knle' 'pharmacy' 'gr' 'intel' 'corel'\n",
            " 'prescription' 'iit' 'demokritos' 'rolex' 'xanax' 'macromedia' 'dealer'\n",
            " 'uncertainties' 'valium' 'htmlimg' 'darial' '000000' '0310041' 'lots'\n",
            " 'projections' 'jebel' 'adobe' 'rnd' 'color' 'alt' '161' 'colspan' 'pain'\n",
            " 'readers' 'rx' 'canon' 'export' 'draw' 'fontfont' 'gra' 'speculative'\n",
            " '1226030' 'gold' 'pro' 'logos' 'wi' 'toshiba' 'china' '1933' 'spam'\n",
            " 'vicodin' 'itoy' 'viewsonic' 'ooking' '1618' 'cellpadding' 'weight'\n",
            " 'hewlett' '4176' 'pill' 'robotics' 'soma' 'resellers' '8834464' '8834454'\n",
            " 'apc' 'intellinet' 'aopen' 'iomega' 'enquiries' 'customerservice'\n",
            " 'targus' 'packard' 'tr' 'uae' 'dealers' 'spain' 'nomad' '1934' 'drug'\n",
            " 'muscle' 'abdv' 'zonedubai' 'eogi' 'aeor' 'doctors' 'inherent' 'wysak'\n",
            " 'emirates' 'cheap' 'health' 'border' 'illustrator' 'hottlist' 'oem'\n",
            " 'apple' 'ffffff' 'ce' 'verdana' 'sex' 'gif' 'resuits' 'graphics' 'mining'\n",
            " 'studio' 'differ' 'materia' 'predictions' 'arial' 'waste' 'cellspacing'\n",
            " 'yap' 'male' 'phentermine' 'tirr' 'cf' 'wiil' 'construed' 'otcbb'\n",
            " 'atleast' 'materially' 'kin' '2005' 'vi' 'anticipates' 'erections'\n",
            " 'artprice' 'deciding' 'featured' 'prescriptions' 'sofftwaares' 'ali' 'ur'\n",
            " 'sir' 'discreet' 'gains' 'dose' 'cia' 'assurance' 'distributorjebel'\n",
            " 'nigeria' 'spur' 'serial' 'ambien' 'creative' 'align' 'stocks' 'aerofoam'\n",
            " 'der' 'penis' 'emerson' 'bingo' 'ffffffstrongfont' 'mai' 'style'\n",
            " 'anxiety' 'brbr' 'prozac' 'undervalued' 'epson' 'fontbr' 'notebook'\n",
            " 'levitra' 'es' 'iso' 'risks' 'alcohol' 'xm' 'erection' 'lasts' 'effects'\n",
            " 'vlagra' 'technoiogies' '124' 'couid']\n",
            "\n",
            "Top 200 hammiest words:\n",
            " ['enron' 'ect' 'meter' 'hpl' 'daren' 'mmbtu' 'xls' 'pec' 'sitara' 'hou'\n",
            " 'volumes' 'ena' 'forwarded' 'melissa' 'tenaska' 'teco' 'nom' '2001' 'pat'\n",
            " 'aimee' 'actuals' 'noms' 'hsc' 'susan' 'cotten' 'chokshi' 'nomination'\n",
            " 'fyi' 'pipeline' 'wellhead' 'eastrans' 'clynes' 'hplc' '713'\n",
            " 'counterparty' 'pefs' 'bob' 'nominations' 'cec' 'gcs' 'lannou' 'txu'\n",
            " 'farmer' 'hplno' 'rita' 'weissman' 'cc' 'equistar' 'enronxgate' 'iferc'\n",
            " 'scheduled' 'spreadsheet' 'wynne' 'allocated' 'entex' 'path' 'buyback'\n",
            " 'fuels' 'hplo' 'lisa' 'scheduling' 'pops' 'anita' 'calpine' 'gco'\n",
            " 'darren' 'clem' 'steve' 'aep' 'katy' 'tu' 'flowed' 'follows' 'sherlyn'\n",
            " 'donna' 'lloyd' 'midcon' 'pm' 'redeliveries' 'jackie' 'gary' 'vance'\n",
            " 'papayoti' 'meters' 'cornhusker' 'luong' 'howard' 'pg' 'lsk' 'revision'\n",
            " 'julie' 'utilities' '281' 'bryan' 'dfarmer' 'ees' 'reinhardt' 'hplnl'\n",
            " 'cleburne' 'valero' 'unify' 'outage' 'poorman' 'victor' 'methanol' '6353'\n",
            " 'tap' 'baumbach' 'devon' 'lsp' 'lamphier' 'herod' 'liz' 'schumack'\n",
            " 'enserch' 'employee' '098' 'boas' 'megan' 'meyers' 'allocation'\n",
            " 'deliveries' 'easttexas' 'ami' 'enrononline' 'invoice' 'withers' 'taylor'\n",
            " 'robert' 'bellamy' 'fred' 'gpgfin' 'avila' 'pathed' 'duke' 'spoke'\n",
            " 'mccoy' 'cernosek' 'oasis' 'carlos' 'kevin' '1266' 'saturday' '853'\n",
            " 'riley' 'tejas' 'waha' 'katherine' 'kcs' 'graves' 'logistics' 'revised'\n",
            " 'paso' '345' 'eileen' 'hakemack' 'mm' 'ponton' 'cdnow' 'hesco' 'cp'\n",
            " 'reliantenergy' 'sandi' 'btu' 'mckay' 'gomes' 'chad' '0435' 'superty'\n",
            " 'lamadrid' '4179' 'tisdale' 'neon' 'lauri' 'interconnect' 'aepin'\n",
            " 'neuweiler' 'herrera' 'attached' 'panenergy' 'acton' 'tess' 'deal'\n",
            " 'rodriguez' 'mops' 'holmes' 'coastal' 'imbalance' 'stacey'\n",
            " 'availabilities' 'eol' 'pinion' 'heidi' 'camp' 'brenda' 'mary'\n",
            " 'origination' 'charlene' 'billed' 'lee']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# # Combine spam and ham words\n",
        "# top_words = np.concatenate((spam_words[:200], ham_words[:200]))\n",
        "\n",
        "# # Get the feature vectors for the top words\n",
        "# top_word_vectors = np.array([vectorizer.transform([word]).toarray()[0] for word in top_words])\n",
        "\n",
        "# # Apply t-SNE to project the word vectors into 2D\n",
        "# tsne = TSNE(n_components=2, random_state=random_seed)\n",
        "# word_vectors_2d = tsne.fit_transform(top_word_vectors)\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Create a scatter plot of the t-SNE projected word vectors\n",
        "# plt.figure(figsize=(12, 12))\n",
        "\n",
        "# # Plot spam words in red\n",
        "# plt.scatter(word_vectors_2d[:200, 0], word_vectors_2d[:200, 1], c='red', label='Spam')\n",
        "\n",
        "# # Plot ham words in blue\n",
        "# plt.scatter(word_vectors_2d[200:, 0], word_vectors_2d[200:, 1], c='blue', label='Ham')\n",
        "\n",
        "# # Set plot title and axis labels\n",
        "# plt.title('t-SNE Clustering of Top Spam and Ham Words')\n",
        "# plt.xlabel('t-SNE Dimension 1')\n",
        "# plt.ylabel('t-SNE Dimension 2')\n",
        "\n",
        "# # Add a legend\n",
        "# plt.legend()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "g_wKWgzsdIZ4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OJFrq33oKr"
      },
      "source": [
        "**Look at the words that best distinguish SPAM and HAM:**\n",
        "1. How many of your words showed up in the SPAM and HAM top 200 predictive words?\n",
        "2. Are they what you would have expected?\n",
        "3. Based on this, what can you say about the differences between how people make prediction and ML algorithms make predictions?\n",
        "4. Does this make you more confident or less confident in ML predictions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgog1Tcg3oKr"
      },
      "source": [
        "**Discuss here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRBVr7773oKr"
      },
      "source": [
        "# 2) Bayesian Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aycL1C93oKr"
      },
      "source": [
        "In this problem, we'll be using the `ASIA` dataset, which showcases the reltionships between travel, smoking, etc. and the probabilty of having various conditions. We will be using the [pomegranate](https://pomegranate.readthedocs.io/en/latest/) library to handle the heavy lifting for Bayes Nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4t7iEI3oKr"
      },
      "source": [
        "![](./asia_data.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nDjVtV3oKr"
      },
      "source": [
        "## 2.1) Exploring Bayes Nets (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwcDLa-3oKr"
      },
      "source": [
        "First go here: https://www.bayesserver.com/examples/networks/asia\n",
        "\n",
        "Try checking different boxes and seeing how the model updates. When you check a box, you're \"given\" a specific value for that node. For example, checking \"True\" for \"Visit to Asia\" means the patient has visited Asia, but we don't know the other probabilities yet. The new probabilities are \"given\" that you've visited Asia.\n",
        "\n",
        "Now answer the following questions. For each one, first make a **prediction** about how the model will change, and the try it to see if you're right. Write down your prediction and then the actual answer. If your prediction differs than the actual answer, try and discuss why.\n",
        "\n",
        "**After each question, uncheck all boxes.**\n",
        "\n",
        "1. If you set the value of Visit to Asia, which nodes will update?\n",
        "2. If you set the value of XRay Result, which nodes with update?\n",
        "3. First set the value for Has Tuberculosis. If you then set the value for Visit to Asia, which nodes will update?\n",
        "4. First set the value for Tuberculosis or Cancer. If you then set the value for Dyspnea, which nodes will update?\n",
        "5. If you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "6. First set the value for Tuberculosis or Cancer. Now if you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "\n",
        "If you wish to understand more about conditional independence and D-Separation, go here:  https://www.youtube.com/watch?v=_R_RYn5KelA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh6nOQi03oKs"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrtNDwaW3oKs"
      },
      "source": [
        "## 2.2) Building the Bayes Net (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlDKW5G3oKs"
      },
      "source": [
        "Just like in class, we can define the initial structure and conditional probability tables for the Bayes Net using our expert knowledge of the scenario (in this case, given to use by experts).\n",
        "\n",
        "For example, the first table gives the probability of having TB give that you have (or have not) visited Asia.\n",
        "\n",
        "| Asia | HasTB | P(HasTB\\|Asia) |\n",
        "| ---- | ----- | ------------- |\n",
        "| T | T | 0.05 |\n",
        "| T | F | 0.95 |\n",
        "| F | T | 0.01 |\n",
        "| F | F | 0.99 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ToHUzunj3oKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4e3b0c-0b0d-4fc3-841d-621a30d5b3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pomegranate in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (2.0.1+cu118)\n",
            "Requirement already satisfied: apricot-select>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (0.6.1)\n",
            "Requirement already satisfied: networkx>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from pomegranate) (3.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.10/dist-packages (from apricot-select>=0.6.1->pomegranate) (0.56.4)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from apricot-select>=0.6.1->pomegranate) (4.65.0)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from apricot-select>=0.6.1->pomegranate) (1.3.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pomegranate) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pomegranate) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pomegranate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pomegranate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pomegranate) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pomegranate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pomegranate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->pomegranate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->pomegranate) (16.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->apricot-select>=0.6.1->pomegranate) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.43.0->apricot-select>=0.6.1->pomegranate) (67.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->pomegranate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->pomegranate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pgmpy\n",
            "  Downloading pgmpy-0.1.22-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.5.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.0.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pgmpy) (2.0.1+cu118)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from pgmpy) (0.13.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pgmpy) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pgmpy) (1.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from pgmpy) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pgmpy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pgmpy) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pgmpy) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pgmpy) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pgmpy) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->pgmpy) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pgmpy) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->pgmpy) (16.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->pgmpy) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pgmpy) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pgmpy) (1.3.0)\n",
            "Installing collected packages: pgmpy\n",
            "Successfully installed pgmpy-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install keras\n",
        "# !pip install Cython\n",
        "# !pip uninstall pomegranate\n",
        "!pip install pomegranate\n",
        "!pip install pgmpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "f9d6b254-7abc-46fb-f2d7-4662fc3fa776",
        "id": "bsZ1ymPW4DgR"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-55bfe39937a2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpomegranate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpomegranate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscreteDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConditionalProbabilityTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print( os.path.dirname(pomegranate.__file__))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DiscreteDistribution' from 'pomegranate.distributions' (/usr/local/lib/python3.10/dist-packages/pomegranate/distributions/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pomegranate\n",
        "from pomegranate.distributions import DiscreteDistribution, ConditionalProbabilityTable\n",
        "#print( os.path.dirname(pomegranate.__file__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "VTqq_CrP3oKs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "716b94cc-9a0c-4dc7-fbd4-24357042126c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-b821fa4ffeab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# First, we define our top level nodes with their base probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpomegranate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpomegranate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscreteDistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvisit_to_asia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'T'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DiscreteDistribution' from 'pomegranate' (/usr/local/lib/python3.10/dist-packages/pomegranate/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# First, we define our top level nodes with their base probabilities.\n",
        "import pomegranate\n",
        "from pomegranate import DiscreteDistribution\n",
        "\n",
        "visit_to_asia = DiscreteDistribution({'T':0.01, 'F':0.99})\n",
        "smoke = DiscreteDistribution({'T':0.5, 'F':0.5})\n",
        "\n",
        "# Now, we have to fill in all of the conditional probability tables for the other nodes\n",
        "\n",
        "has_tb = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Asia? #HasTB #Probability\n",
        "        [\"T\",\"T\",0.05],\n",
        "        [\"T\",\"F\",0.95],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99],\n",
        "    ], [visit_to_asia])\n",
        "\n",
        "\n",
        "has_lung_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke? \n",
        "        [\"T\",\"T\",0.1],\n",
        "        [\"T\",\"F\",0.9],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99]\n",
        "    ], [smoke])\n",
        "\n",
        "\n",
        "has_bc = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke?\n",
        "        [\"T\",\"T\",0.6],\n",
        "        [\"T\",\"F\",0.4],\n",
        "        \n",
        "        [\"F\",\"T\",0.3],\n",
        "        [\"F\",\"F\",0.7]\n",
        "    ], [smoke])\n",
        "\n",
        "tb_or_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Lung? TB? \n",
        "        [\"T\",\"T\",\"T\",1],\n",
        "        [\"T\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",1],\n",
        "        [\"T\",\"F\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",1],\n",
        "        [\"F\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0],\n",
        "        [\"F\",\"F\",\"F\",1]\n",
        "    ], [has_lung_cancer,has_tb])\n",
        "\n",
        "x_ray_abnormal = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #TB or Cancer?\n",
        "        [\"T\",\"T\",0.98],\n",
        "        [\"T\",\"F\",0.02],\n",
        "        \n",
        "        [\"F\",\"T\",0.05],\n",
        "        [\"F\",\"F\",0.95]\n",
        "    ], [tb_or_cancer])\n",
        "\n",
        "dyspnea = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #BC\n",
        "        [\"T\",\"T\",\"T\",0.9],\n",
        "        [\"T\",\"T\",\"F\",0.1],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",0.8],\n",
        "        [\"T\",\"F\",\"F\",0.2],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",0.7],\n",
        "        [\"F\",\"T\",\"F\",0.3],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0.1],\n",
        "        [\"F\",\"F\",\"F\",0.9]\n",
        "    ], [has_bc, tb_or_cancer])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tried to dop this using pgmpy instead of pomegranate...no dice\n",
        "# Installing conda on my other machine to quarantine it (Ananconda) from my regular Python development env\n",
        " \n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from pgmpy.models import BayesianModel\n",
        "# from pgmpy.factors.discrete import TabularCPD\n",
        "\n",
        "# # Define the model structure.\n",
        "# model = BayesianModel([('visit_to_asia', 'has_tb'), \n",
        "#                        ('smoke', 'has_lung_cancer'), \n",
        "#                        ('smoke', 'has_bc'), \n",
        "#                        ('has_lung_cancer', 'tb_or_cancer'), \n",
        "#                        ('has_tb', 'tb_or_cancer'), \n",
        "#                        ('tb_or_cancer', 'x_ray_abnormal'),\n",
        "#                        ('has_bc', 'dyspnea'),\n",
        "#                        ('tb_or_cancer', 'dyspnea')])\n",
        "\n",
        "# # Define the CPDs:\n",
        "# cpd_visit_to_asia = TabularCPD(variable='visit_to_asia', variable_card=2, values=[[0.99], [0.01]], \n",
        "#                                state_names={'visit_to_asia': ['F', 'T']})\n",
        "\n",
        "\n",
        "# cpd_smoke = TabularCPD(variable='smoke', variable_card=2, values=[[0.5], [0.5]], \n",
        "#                        state_names={'smoke': ['F', 'T']})\n",
        "\n",
        "\n",
        "# cpd_has_tb = TabularCPD(variable='has_tb', variable_card=2, \n",
        "#                         values=[[0.95, 0.99], [0.05, 0.01]],  # corrected values\n",
        "#                         evidence=['visit_to_asia'], \n",
        "#                         evidence_card=[2],\n",
        "#                         state_names={'has_tb': ['F', 'T'], 'visit_to_asia': ['F', 'T']})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Add CPDs to the model\n",
        "# model.add_cpds(cpd_visit_to_asia, cpd_smoke, cpd_has_tb)\n",
        "\n",
        "# # Validate the model\n",
        "# model.check_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "hN9JcB1XgwoR",
        "outputId": "bbf9eca0-5a8a-4657-8fa0-4cd691170b7c"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-384d171aa2a7>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Validate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pgmpy/models/BayesianNetwork.py\u001b[0m in \u001b[0;36mcheck_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Check if a CPD is associated with every node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcpd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No CPD associated with {node}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# Check if the CPD is an instance of either TabularCPD or ContinuousFactor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No CPD associated with has_lung_cancer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wGmUvsL53oKs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "27b4c18e-d741-43b7-857b-0d3f1f572a67"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-24b4e1a3f524>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Next we have to create all the nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0masia_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisit_to_asia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"asia\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtb_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhas_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msmoke_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"smoke\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlung_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhas_lung_cancer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lung\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
          ]
        }
      ],
      "source": [
        "# Next we have to create all the nodes\n",
        "asia_node = Node(visit_to_asia, name=\"asia\")\n",
        "tb_node = Node(has_tb, name=\"tb\")\n",
        "smoke_node = Node(smoke, name=\"smoke\")\n",
        "lung_node = Node(has_lung_cancer, name=\"lung\")\n",
        "bronc_node = Node(has_bc, name=\"bc\")\n",
        "either_node = Node(tb_or_cancer, name=\"either\")\n",
        "xray_node = Node(x_ray_abnormal,name=\"xray\")\n",
        "dysp_node = Node(dyspnea, name=\"dysp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "S40oXQvn3oKs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "3bc6f3c5-7930-4a4d-c4b4-6dac265363c1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d37e0b5cae8c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now we init the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ASIA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model.add_states(asia_node,\n\u001b[1;32m      4\u001b[0m                  \u001b[0mtb_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0msmoke_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BayesianNetwork' is not defined"
          ]
        }
      ],
      "source": [
        "# Now we init the model\n",
        "model = BayesianNetwork(\"ASIA\")\n",
        "model.add_states(asia_node,\n",
        "                 tb_node,\n",
        "                 smoke_node,\n",
        "                 lung_node,\n",
        "                 bronc_node,\n",
        "                 either_node,\n",
        "                 xray_node,\n",
        "                 dysp_node)\n",
        "\n",
        "# Add all of the correct edges \n",
        "model.add_edge(asia_node, tb_node)\n",
        "\n",
        "model.add_edge(smoke_node, bronc_node)\n",
        "model.add_edge(smoke_node, lung_node)\n",
        "\n",
        "model.add_edge(tb_node,either_node)\n",
        "model.add_edge(lung_node,either_node)\n",
        "\n",
        "model.add_edge(either_node, xray_node)\n",
        "model.add_edge(either_node, dysp_node)\n",
        "\n",
        "model.add_edge(bronc_node, dysp_node)\n",
        "\n",
        "# And then commit our changes\n",
        "model.bake()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "uoO5th503oKs"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the model structure\n",
        "def print_model_structure(model, features):\n",
        "    for i in range(len(features)):\n",
        "        parents = [features[pi] for pi in model.structure[i]]\n",
        "        print(f'Node \"{features[i]}\" has parents: {parents}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2OSXGErC3oKs"
      },
      "outputs": [],
      "source": [
        "# We'll keep our features in this order for consistency\n",
        "features = [\n",
        "    \"Visit to Asia\",\n",
        "    \"Has TB\",\n",
        "    \"Smoker\",\n",
        "    \"Has Lung Cancer\",\n",
        "    \"Has Bronchitis\",\n",
        "    \"TB or Cancer\",\n",
        "    \"XRay Abnormal\",\n",
        "    \"Dyspnea\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gQ5vxsyM3oKs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "16ca98cd-ff38-4627-d85d-2f04e0e1cad1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-81416d3be867>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's make sure the structure of our newly created model is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint_model_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Let's make sure the structure of our newly created model is correct\n",
        "print_model_structure(model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPamBaC3oKs"
      },
      "source": [
        "## 2.3) Predictions (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdmmYfwg3oKs"
      },
      "source": [
        "`predict` allows us to do inference based off of the data. It chooses the values that are the most likely.\n",
        "\n",
        "For example, let's say we have a patient who has an abnormal X-ray, but is not a smoker and hasn't visited Asia. We can then infer the most likely values for all of the other variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Old8Bu-33oKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "6c0e1989-6e56-4358-8f50-0a51be95238f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-29776d824d44>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.predict([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.predict([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJwMizsQ3oKt"
      },
      "outputs": [],
      "source": [
        "# Now let's say they *were* a smoker. See how it changes?\n",
        "model.predict([\n",
        "    [\"F\",None,\"T\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVPgog63oKt"
      },
      "source": [
        "We can get a little more detail and check out the actual probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXCd4FWR3oKt"
      },
      "outputs": [],
      "source": [
        "def pretty_results(results):\n",
        "    for i,dist in enumerate(results):\n",
        "        print(features[i])\n",
        "        if isinstance(dist,str):\n",
        "            print(dist)\n",
        "        else:\n",
        "            print(dist.parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo8xV7iX3oKt"
      },
      "outputs": [],
      "source": [
        "res = model.predict_proba([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])\n",
        "pretty_results(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyXMsO-3oKt"
      },
      "source": [
        "Use the Bayes net to calcualte the following probabilities:\n",
        "\n",
        "1. $P(xray=true | TBorCancer=true)$\n",
        "2. $P(xray=true | TBorCancer=true, TB=true)$\n",
        "3. $P(TB=true)$\n",
        "4. $P(TB=true | smoke=false)$\n",
        "5. $P(TB=true | smoke=false, TBorCancer=true)$\n",
        "\n",
        "What values are equivalent? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA2KHRPl3oKt"
      },
      "source": [
        "**Write your answers here:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5.\n",
        "\n",
        "What values are equivalent? Why?\n",
        "\n",
        "And write code below to help you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBK7Kl-Y3oKt"
      },
      "outputs": [],
      "source": [
        "# As a reminder, here are the indices of the features\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXtRvJ363oKt"
      },
      "outputs": [],
      "source": [
        "# You can modify this code to help you\n",
        "pretty_results(model.predict_proba([\n",
        "    [None,None,'F',None,None,'T',None,None]\n",
        "])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvFDNx3H3oKt"
      },
      "source": [
        "## 2.4) Evaluating Bayes Nets (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dOhNguE3oKt"
      },
      "source": [
        "Let's see how well this net is on inferencing from data. We're going to remove the Bronchitis column from this dataset, and see if our net can predict what the missing value should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzPG51iF3oKt"
      },
      "outputs": [],
      "source": [
        "# Some data we will use to generate our probabilities\n",
        "asia_data = pd.read_csv(\"Asia10k.csv\")\n",
        "asia_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_23SlnAX3oKu"
      },
      "outputs": [],
      "source": [
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTJ5Cqjn3oKu"
      },
      "outputs": [],
      "source": [
        "# Let's make sure we're consistant with our labels\n",
        "asia_data = asia_data.replace(\"no\", \"F\").replace(\"yes\", \"T\")\n",
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y4ltbBJ3oKu"
      },
      "outputs": [],
      "source": [
        "values = asia_data.values.copy()\n",
        "indices = np.random.choice(asia_data.index, 1000)\n",
        "values = values[indices]\n",
        "values[:,4] = None\n",
        "values[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DJTLoG03oKu"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMAVAWm3oKu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDkwBqa3oKu"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(asia_data.values[indices,4],np.array(predictions)[:,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4hBVGX3oKu"
      },
      "outputs": [],
      "source": [
        "print(classification_report(asia_data.values[indices,4],np.array(predictions)[:,4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHJubZb3oKu"
      },
      "source": [
        "## 2.5) Fitting a Bayes Net to Data (Group)\n",
        "\n",
        "In many applications, you may have a general idea of the structure of the Bayes Net, but do not have a list of probabilities. Luckily, given some data, we can fill out the probabilities in a given net. **Note:** You may only get similar results to the previous method, since it turns out this data was *simulated* from the given conditional probabilities. So, one would expect that the model would learn parameters like the ones we've given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83gWfLiD3oKu"
      },
      "outputs": [],
      "source": [
        "fitted_model = model.fit(asia_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_0JlNi3oKu"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the probability distributions\n",
        "def print_distributions(model):\n",
        "    for i, state in enumerate(fitted_model.states):\n",
        "        print(features[i])\n",
        "        states = state.distribution.parameters[0]\n",
        "        if len(states)>1:\n",
        "            for state in states:\n",
        "                print(state)\n",
        "        #print(state.distribution.parameters[0])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_8lKDiQ3oKu"
      },
      "outputs": [],
      "source": [
        "print_distributions(fitted_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB0ccZug3oKu"
      },
      "source": [
        "**Take a look at the learned probability distributions. Are they similar to \"expert\" ones given in the previous problem?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8751vMuM3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlViQF7V3oKv"
      },
      "source": [
        "Now, perform the same evaluation that you did in the previous problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNI7EMf53oKv"
      },
      "outputs": [],
      "source": [
        "# Remove Some other column other than Bronchitis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOdZ0cKE3oKv"
      },
      "outputs": [],
      "source": [
        "# Make the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdqbnXB23oKv"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0lXJgal3oKv"
      },
      "outputs": [],
      "source": [
        "# Classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPgf3l13oKv"
      },
      "source": [
        "How well did it perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQWm4yT3oKv"
      },
      "source": [
        "## 2.6) Learning Structure from Data (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIlFb6YZ3oKv"
      },
      "source": [
        "Now, the most interesting problem is when we only have data, but we don't know the structure of the data (however, we still have a reason to believe that *it can be reperesented as Bayes Net*). Luckily, pomegranate has the ability to solve this problem as well. Given a dataset, we can use `from_samples` to build a Bayes net, structure and all, from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQw2ci3r3oKv"
      },
      "outputs": [],
      "source": [
        "learned_model = BayesianNetwork.from_samples(asia_data, algorithm='exact')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_afVm0o43oKv"
      },
      "outputs": [],
      "source": [
        "print_model_structure(learned_model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyEpFS_3oKv"
      },
      "source": [
        "**Compare the model structure from experts vs learned from the data:**\n",
        "\n",
        "1. Draw out both models (the one you made earlier and the one learned) on a piece of paper.\n",
        "2. What are the differences you observe?\n",
        "3. Why might a model learned from the data have a different structure? Are some influences (edges) in the model more or less important than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnzxe7sv3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7GIWIFG3oKw"
      },
      "source": [
        "## 2.7) BYOB: Build Your Own Bayes Net (Group, if time permits)\n",
        "If you find yourself with some extra time after this portion, consider **building a Bayes net that represents something in your daily life**. It could be the effect of traffic on a morning commute, deciding what to do for dinner, etc. It can be very small, only around 3-5 nodes probably (conditional probability tables are a pain!). Then play around with predictions and probabilities to see how various factors impact your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzIWCRVT3oKw"
      },
      "outputs": [],
      "source": [
        "# Start building!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}