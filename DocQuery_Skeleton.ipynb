{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/DocQuery_Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDlIfVreCCEo",
        "outputId": "a14a8544-4e73-4f68-952c-fe6eb926dd0f"
      },
      "id": "CDlIfVreCCEo",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c880caf1",
      "metadata": {
        "id": "c880caf1"
      },
      "outputs": [],
      "source": [
        "# DocQuery-example.py\n",
        "# Collin Lynch and Travis Martin\n",
        "# 9/8/2021\n",
        "\n",
        "# The DocQuery code is taked with taking a query and a set of\n",
        "# saved documents and then returning the document that is closest\n",
        "# to the query using either TF/IDF scoring or the sum vector.\n",
        "# When called this code will load the docs into memory and deal\n",
        "# with the distance one at a time.\n",
        "\n",
        "# Imports\n",
        "# ---------------------------------------------\n",
        "import spacy\n",
        "import os\n",
        "import scipy.spatial\n",
        "import nltk\n",
        "import gensim as gm\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "#load the spacy model\n",
        "#spacy.cli.download(\"en_core_web_sm\")  #you may have to run this line the first time through\n",
        "MODEL = spacy.load(\"en_core_web_sm\")\n",
        "np.random.seed(seed=42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3956831e",
      "metadata": {
        "id": "3956831e"
      },
      "outputs": [],
      "source": [
        "# Core Code.\n",
        "# ---------------------------------------------\n",
        "\"\"\"\n",
        "input: directory for the url file and url file name\n",
        "1. open the url file and read contents 1 line at a time\n",
        "2. append the urls to a list\n",
        "3. go through the list of urls and open each page\n",
        "4. clean the text from each page\n",
        "5. store the text from the webpage and tokenized word in separate dictionaries\n",
        "output: dictionaries with page text and tokenized words\n",
        "\"\"\"\n",
        "def load_webpages(Directory, url_file):\n",
        "\n",
        "    Loaded_Docs_words = {}\n",
        "    Loaded_Docs_page = {}\n",
        "    webpage_names = []\n",
        "\n",
        "    with open(Directory + URL_File, 'r') as InFile:\n",
        "        website = InFile.readline()[:-1]\n",
        "        while website:\n",
        "            webpage_names.append(website)\n",
        "            website = InFile.readline()[:-1]\n",
        "\n",
        "    for page in webpage_names:\n",
        "        Req = requests.get(page)\n",
        "        SoupText = BeautifulSoup(Req.text, features=\"lxml\")\n",
        "        PageText = SoupText.get_text()\n",
        "        Words = nltk.tokenize.word_tokenize(PageText)\n",
        "        Loaded_Docs_words[page] = Words\n",
        "        Loaded_Docs_page[page] = PageText\n",
        "\n",
        "    return Loaded_Docs_words, Loaded_Docs_page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b57c7688",
      "metadata": {
        "id": "b57c7688"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: raw page text, spacey model\n",
        "1. iterate through the raw text pages and pass them into the spacey model\n",
        "2. pass out the modeled text\n",
        "output: spacey modeled text\n",
        "\"\"\"\n",
        "def build_model(Loaded_Docs_page, MODEL):\n",
        "\n",
        "    spacey_model_page = {}\n",
        "\n",
        "    for key in Loaded_Docs_page:\n",
        "        raw_page = Loaded_Docs_page[key]\n",
        "        doc = MODEL(raw_page)\n",
        "        spacey_model_page[key] = doc\n",
        "\n",
        "    return spacey_model_page\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7d1345e",
      "metadata": {
        "id": "f7d1345e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: spacey model of the raw page text\n",
        "1. iterate through the webpages\n",
        "2. create a summed vector for the words in the webpage\n",
        "3. Normalize the doc vector by the number of words in the doc\n",
        "output: a vector representing the webpage\n",
        "\"\"\"\n",
        "def find_doc_vec(spacey_model_page):\n",
        "    webpage_vectors = {}\n",
        "    shape = (96,)\n",
        "\n",
        "    for webpage in spacey_model_page:\n",
        "        temp_array = np.empty(shape)\n",
        "        word_count = 0\n",
        "        for word in spacey_model_page[webpage]:\n",
        "            temp_array += word.vector\n",
        "            word_count += 1\n",
        "\n",
        "        webpage_vectors[webpage] = temp_array/word_count\n",
        "    return webpage_vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6320dafb",
      "metadata": {
        "id": "6320dafb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: vectorized webpages\n",
        "1. initialize the first two webpages as the closest\n",
        "2. iterate through all possible webpage combinations (order does not matter)\n",
        "3. compute the manhattan, euclidean, and cosine distance for each pair\n",
        "4. compare the different distances to find the smallest\n",
        "5. report the smallest distances\n",
        "output: print the 2 closest webpages for each distance metric\n",
        "\"\"\"\n",
        "def find_closest_two_webpages(webpage_vectors):\n",
        "    low_key1 = list(webpage_vectors.keys())[0]\n",
        "    low_key2 = list(webpage_vectors.keys())[1]\n",
        "    low_value1 = webpage_vectors[low_key1]\n",
        "    low_value2 = webpage_vectors[low_key2]\n",
        "    low_key1E = low_key1\n",
        "    low_key2E = low_key2\n",
        "    low_key1C = low_key1\n",
        "    low_key2C = low_key2\n",
        "\n",
        "    lowest_distance_manh = np.sum(abs(low_value1-low_value2))\n",
        "    lowest_distance_eucl = np.linalg.norm(low_value1 - low_value2)\n",
        "    lowest_distance_cos = np.dot(low_value1, low_value2)/(np.linalg.norm(low_value1)*np.linalg.norm(low_value2))\n",
        "\n",
        "    outer_index = 0\n",
        "    while outer_index < len(list(webpage_vectors.keys()))-1:\n",
        "        inner_index = outer_index + 1\n",
        "        while inner_index < len(list(webpage_vectors.keys())):\n",
        "            key1 = list(webpage_vectors.keys())[outer_index]\n",
        "            key2 = list(webpage_vectors.keys())[inner_index]\n",
        "            value1 = webpage_vectors[key1]\n",
        "            value2 = webpage_vectors[key2]\n",
        "\n",
        "            current_distance_manh = np.sum(abs(value1-value2))\n",
        "            current_distance_eucl = np.linalg.norm(value1 - value2)\n",
        "            current_distance_cos = np.dot(value1, value2)/(np.linalg.norm(value1)*np.linalg.norm(value2))\n",
        "\n",
        "            if current_distance_manh <= lowest_distance_manh:\n",
        "                low_key1 = key1\n",
        "                low_key2 = key2\n",
        "\n",
        "            if current_distance_eucl <= lowest_distance_eucl:\n",
        "                low_key1E = key1\n",
        "                low_key2E = key2\n",
        "\n",
        "            if current_distance_cos <= lowest_distance_cos:\n",
        "                low_key1C = key1\n",
        "                low_key2C = key2\n",
        "\n",
        "            low_value1 = webpage_vectors[low_key1]\n",
        "            low_value2 = webpage_vectors[low_key2]\n",
        "            low_value1E = webpage_vectors[low_key1E]\n",
        "            low_value2E = webpage_vectors[low_key2E]\n",
        "            low_value1C = webpage_vectors[low_key1C]\n",
        "            low_value2C = webpage_vectors[low_key2C]\n",
        "\n",
        "            inner_index += 1\n",
        "        outer_index += 1\n",
        "\n",
        "    print('Two closest webpages using Manhattan distance:', low_key1, 'and', low_key2)\n",
        "    print('Two closest webpages using Eulcidean distance:', low_key1E, 'and', low_key2E)\n",
        "    print('Two closest webpages using Cosine distance:', low_key1C, 'and', low_key2C)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8319d3ad",
      "metadata": {
        "id": "8319d3ad"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: query string, spacey model, vectorized webpages\n",
        "1. split the query string into a list of words\n",
        "2. find the vector for each word and store it\n",
        "3. iterate through the webpages and compare the vectors to the query\n",
        "4. I used manhattan, euclidean, and cosine distances\n",
        "5. print the results to the screen\n",
        "output: print the closest webpage to the query\n",
        "\"\"\"\n",
        "\n",
        "def find_closest_page_to_query(Query_String, MODEL, webpage_vectors):\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    Add your code here\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    low_keyM = 'Find this value'\n",
        "    low_keyE = 'Find this value'\n",
        "    low_keyC = 'Find this value'\n",
        "\n",
        "\n",
        "    print('Closest Manhattan distance webpage to query is:', low_keyM)\n",
        "    print('Closest Euclidean distance webpage to query is:', low_keyE)\n",
        "    print('Closest Cosine distance webpage to query is:', low_keyC)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b808aec3",
      "metadata": {
        "id": "b808aec3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: raw loaded docs in a dictionary\n",
        "1. store all the keys of the docs for later\n",
        "2. store all the text from the docs in a list\n",
        "3. use gensim to create a BOW then use that to create TFIDF ectors\n",
        "4. iterate through the tfidf vectors and put the words and scores into a dictionary\n",
        "5. iterate through all tfidf dictionaries and put them into one total dictionary\n",
        "output: return the dictionary with all keys and tfidf values\n",
        "\"\"\"\n",
        "def compute_tfidf_value(Loaded_Docs_page):\n",
        "\n",
        "    all_doc_list = []\n",
        "    key_list = list(Loaded_Docs_page.keys())\n",
        "\n",
        "    for key in Loaded_Docs_page:\n",
        "        all_doc_list.append(Loaded_Docs_page[key])\n",
        "\n",
        "    doc_tokenized = [gm.utils.simple_preprocess(doc) for doc in all_doc_list]\n",
        "    dictionary = gm.corpora.Dictionary()\n",
        "    BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
        "    tfidf = gm.models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
        "\n",
        "    tfidf_list = []\n",
        "    for doc in tfidf[BoW_corpus]:\n",
        "        tfidf_dict = {}\n",
        "        for id, freq in doc:\n",
        "            word = dictionary[id]\n",
        "            score = freq\n",
        "            tfidf_dict[word] = score\n",
        "\n",
        "        tfidf_list.append(tfidf_dict)\n",
        "\n",
        "    tfidf_dict_all_files = {}\n",
        "    for index in range(len(key_list)):\n",
        "        tfidf_dict_all_files[key_list[index]] = tfidf_list[index]\n",
        "\n",
        "    return tfidf_dict_all_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "30675f3a",
      "metadata": {
        "id": "30675f3a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: tfidf dictionary with all files, number of keywords\n",
        "1. iterate through the tfidf dictionary\n",
        "2. sort each dictionary and print top keywords\n",
        "output: print the number of keywords specified\n",
        "\"\"\"\n",
        "def get_keywords(tfidf_dict_all_files, num_keywords):\n",
        "    for file in tfidf_dict_all_files:\n",
        "        tfidf_dict_all_files[file]\n",
        "        res = dict(sorted(tfidf_dict_all_files[file].items(), key = lambda x: x[1], reverse = True)[:num_keywords])\n",
        "        print(f\"The top {num_keywords} keywords for {file} are  \" + str(res), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e4a4925b",
      "metadata": {
        "id": "e4a4925b"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "input: tfidf dictionary with all files, a query string\n",
        "1. split the query string into a list of words\n",
        "2. iterate through the tfidf dictionaries\n",
        "3. try to find the word in the dictionary and add its score to the total\n",
        "4. store the scores of the query with the webpage in a new dictionary\n",
        "5. find the highest score in this new dictionary\n",
        "6. print the highest value to the screen\n",
        "output: print the highest score for the words in the query\n",
        "\"\"\"\n",
        "def keyword_search(tfidf_dict_all_files, Query_String):\n",
        "\n",
        "    '''\n",
        "    Add your code here\n",
        "    '''\n",
        "\n",
        "\n",
        "    result = 'not found yet'\n",
        "    print('The webpage that is closest to the query is:', result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6ce9c56b",
      "metadata": {
        "id": "6ce9c56b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99236d84",
      "metadata": {
        "id": "99236d84"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initial declarations\n",
        "    URL_File = '/content/drive/MyDrive/corpus-zeta/2023/oit-videos-bios/brad-houston-2023-05-13-apid-oit.txt'\n",
        "    Directory = os.path.join(os.getcwd(), 'webpages')\n",
        "    Query_String = 'The happiest place on earth'\n",
        "    num_keywords = 10\n",
        "\n",
        "    # Open the url file and store the webpage names for later\n",
        "    Loaded_Docs_words, Loaded_Docs_page = load_webpages(Directory, URL_File)\n",
        "\n",
        "    # Load the documents created in the doc downloader program\n",
        "    spacey_model_page = build_model(Loaded_Docs_page, MODEL)\n",
        "\n",
        "    # Create doc vector using the spacey model\n",
        "    webpage_vectors = find_doc_vec(spacey_model_page)\n",
        "\n",
        "    # Iterate through all available webpages and find two closest\n",
        "    find_closest_two_webpages(webpage_vectors)\n",
        "\n",
        "    # Find the closest webpage to the query\n",
        "    find_closest_page_to_query(Query_String, MODEL, webpage_vectors)\n",
        "\n",
        "    # Compute the tfidf scores for all the webpages\n",
        "    tfidf_dict_all_files = compute_tfidf_value(Loaded_Docs_page)\n",
        "\n",
        "    # Iterate through all the webpages and return the top N keywords\n",
        "    get_keywords(tfidf_dict_all_files, num_keywords)\n",
        "\n",
        "    # Search all the documents and find the closest to the query string\n",
        "    keyword_search(tfidf_dict_all_files, Query_String)\n",
        "\n",
        "    #compute the tfidf scores for all the webpages\n",
        "    tfidf_dict_all_files = compute_tfidf_value(Loaded_Docs_page)\n",
        "\n",
        "    #iterate through all the webpages and return the top N keywords\n",
        "    get_keywords(tfidf_dict_all_files, num_keywords)\n",
        "\n",
        "    #search all the documents and find the closest to the query string\n",
        "    keyword_search(tfidf_dict_all_files, Query_String)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}