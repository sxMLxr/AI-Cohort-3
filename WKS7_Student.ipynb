{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelwnau/ai_academy_notebooks/blob/main/WKS7_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kohSsvxj3oKY"
      },
      "source": [
        "# Workshop 7: Bayes Rule Rules\n",
        "\n",
        "In this workshop, we'll be looking at how to use Naive Bayes and Bayes Nets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pfpOZpT36JR",
        "outputId": "78373e91-df0c-4953-def8-820ced757448"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Rdl7Os3oKZ"
      },
      "source": [
        "# 0) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uwRsl8_y3oKZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "# set a seed for reproducibility\n",
        "random_seed = 25\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDRXoMEM3oKa"
      },
      "source": [
        "# 1) Naive Bayes Spam Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AekfdRJp3oKa"
      },
      "source": [
        "One historical use of Naive Bayes is to try and detect [spam emails](https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering). \n",
        "\n",
        "In this exercise, you will be using dataset that of emails from the [Enron Corporation](https://en.wikipedia.org/wiki/Enron_Corpus), an accounting firm that [went bankrupt in 2001 due to an accounting scandal](https://en.wikipedia.org/wiki/Enron_scandal)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KUnQ3Kn3oKa"
      },
      "source": [
        "## 1.1) Exploring the Data (Follow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "J0b-zYmP3oKa",
        "outputId": "8b655388-e0c3-49f3-fe2a-21a618af46b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8e77d7e824fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./enron_emails.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './enron_emails.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"./enron_emails.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-yYqAY23oKa"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cunYeUfS3oKb"
      },
      "source": [
        "Keeping with the theme of meat products, some researchers call emails that are *not spam*, **ham**. \n",
        "\n",
        "To sum up: a **ham** email is a legitimate email, while a **spam** email is unwanted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usZGaAqi3oKb"
      },
      "outputs": [],
      "source": [
        "# Let's look at our distriubtion of spam and ham emails\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAfmrjIe3oKb"
      },
      "source": [
        "Let's explore some of the ham emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bzWi7k3oKb"
      },
      "outputs": [],
      "source": [
        "print(df[df[\"label\"]==\"ham\"].text.iloc[4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDZPf3_D3oKc"
      },
      "source": [
        "And now the spam emails..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcYfLy5f3oKc"
      },
      "outputs": [],
      "source": [
        "print(df[df[\"label\"]==\"spam\"].text.iloc[18])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4358Uhc3oKc"
      },
      "source": [
        "Try exploring different emails by changing the index in the lines above. **What common traits do you notice accross the ham emails? The spam emails?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Ckb1nD3oKc"
      },
      "source": [
        "## 1.2) Bag of Words (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ52GomW3oKc"
      },
      "source": [
        "Last week we used tf-idf to represent words as feature vectors. However, sometimes simpler methods work just as well (if not better). For this, we'll be using the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation of a piece of text, which is much more interpretable than tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZerWdYep3oKc"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BvnXPhR3oKd"
      },
      "source": [
        "The CountVectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how many times word `m` appears in document `n`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lUP3XGj3oKd"
      },
      "outputs": [],
      "source": [
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMYkf9z53oKd"
      },
      "outputs": [],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F87S6aFI3oKd"
      },
      "source": [
        "A more interpretable view..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJtkLpYz3oKd"
      },
      "outputs": [],
      "source": [
        "print(corpus[0])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtSNslSR3oKd"
      },
      "outputs": [],
      "source": [
        "print(corpus[1])\n",
        "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1PaPk03oKe"
      },
      "source": [
        "Now, if you want to vectorize new data (e.g. test data), then you use the `.transform` function. If the vectorizer encounters a word it hasn't seen before, it will simply ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKEga4MD3oKe"
      },
      "outputs": [],
      "source": [
        "vectorizer.transform([\"This is the coolest document\"]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp85K6mp3oKe"
      },
      "source": [
        "# 1.3) Building and Running the Model (Group)\n",
        "\n",
        "Now that you have all the required tools, build a **Naive Bayes Classifier** and evaluate it on a train and test set. In this instance, [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier, which is most useful for discrete features that use frequency counts (e.g. a bag of words vector)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y-VAl3y3oKe"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e496K1us3oKe"
      },
      "outputs": [],
      "source": [
        "# Create training and test splits - 20% split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtaDlE1I3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize on your training data using BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxhRt-pN3oKe"
      },
      "outputs": [],
      "source": [
        "# Fit the classifier below\n",
        "clf = MultinomialNB()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMuvREFq3oKe"
      },
      "outputs": [],
      "source": [
        "# Vectorize your test data using transform and then predict the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esP-NBkG3oKe"
      },
      "outputs": [],
      "source": [
        "#from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37vUPoIV3oKe"
      },
      "outputs": [],
      "source": [
        "# Print a confusion matrix using confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VPtAci03oKf"
      },
      "outputs": [],
      "source": [
        "# Print a classification report using classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ml1Cy23oKf"
      },
      "source": [
        "## 1.4) Exploring Important Words (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qksJ4daT3oKf"
      },
      "source": [
        "Before you start, predict what words might be more predictive of SPAM or HAM. Make a list below of 5 words you think will be very _predictive_ of an email being SPAM, and 5 words that are predictive of being HAM. Remember this is an office email database from Enron in the 1990s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_S81Onz3oKf"
      },
      "source": [
        "Words the will predict SPAM (junk emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x\n",
        "\n",
        "Words the will predict HAM (real emails):\n",
        "\n",
        "1. x\n",
        "2. x\n",
        "3. x\n",
        "4. x\n",
        "5. x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWA5Gk4B3oKf"
      },
      "source": [
        "**Technical Note: Log Probabilities**: \n",
        "\n",
        "When using probabilistic methods with large datasets, sometimes you get features with extremely small probabilities (e.g. $10^{-10}$). \n",
        "\n",
        "This becomes a problem, because computers aren't really good at doing operations with numbers at this scale. Therefore, in most systems, operations are done on the *log* of the probabilities. \n",
        "\n",
        "This makes calculations much more managable (e.g. $\\log(10^{-10})=-10$). As an added bonus, due to log rules ($log(ab)=log(a)+log(b)$), all multiplications turn into additions, which are easier for the computer.\n",
        "\n",
        "Some general rules of thumb: **the closer to zero a log prob is, the more probabable it is**, and **each time a log prob decreases by one, it's an order of magnitude less probable**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdS2iLHz3oKq"
      },
      "source": [
        "`feature_log_probs` gives us the log probabilities for each word. In notation, each of these are $P(word | class)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU_k5dga3oKq"
      },
      "outputs": [],
      "source": [
        "# Given that a message is ham, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-a_XpxM3oKq"
      },
      "outputs": [],
      "source": [
        "# Given that a message is SPAM, how probable is it for the words to show up?\n",
        "clf.feature_log_prob_[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub__FBPq3oKq"
      },
      "source": [
        "This code will sort all the words by log probability, so that all of the most probable words show up first..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULJwZZO03oKq"
      },
      "outputs": [],
      "source": [
        "spam_args = np.argsort(clf.feature_log_prob_[1])\n",
        "spam_words = np.array(vectorizer.get_feature_names_out())[spam_args]\n",
        "spam_words = np.flip(spam_words)\n",
        "print(spam_words)\n",
        "\n",
        "\n",
        "ham_args = np.argsort(clf.feature_log_prob_[0])\n",
        "ham_words = np.array(vectorizer.get_feature_names_out())[ham_args]\n",
        "ham_words = np.flip(ham_words)\n",
        "print(ham_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaUKsYJk3oKq"
      },
      "outputs": [],
      "source": [
        "spam_words[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIrC07iQ3oKq"
      },
      "outputs": [],
      "source": [
        "ham_words[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fweB7UQ3oKq"
      },
      "source": [
        "However, a more useful way to look at the data is to look at the *ratios* of the probabilities for a given word. For example, if we have the word \"free\":\n",
        "\n",
        "*If an email is spam, there is a 50% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|spam)=0.5$\n",
        "\n",
        "*If an email is ham, there is a 10% probability it will contain the word \"free\"*\n",
        "\n",
        "$P(free|ham)=0.1$\n",
        "\n",
        "*The ratio*\n",
        "\n",
        "$P(free|spam)/P(free|ham)=5$\n",
        "\n",
        "This means that the word *free* is 5x as more likely to show up in spam messages compared to ham messages. So, we can use this to calculate and sort for words that are proportionally more present in spam emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AFQz_xr3oKq"
      },
      "outputs": [],
      "source": [
        "# Since we're operating on logs, division turns into subtraction\n",
        "log_odds = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
        "spam_ham_args = np.argsort(log_odds)\n",
        "spam_ham_words = np.array(vectorizer.get_feature_names_out())[spam_ham_args]\n",
        "spam_ham_words = np.flip(spam_ham_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13UPxtBZ3oKr"
      },
      "source": [
        "Here's some of the \"spammiest\" words..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtmlqiOA3oKr"
      },
      "outputs": [],
      "source": [
        "top_x=200\n",
        "spam_ham_words[0:top_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2JAMfZJ3oKr"
      },
      "source": [
        "Note that words like `td`, `nbsp` and `br` are all HTML tags (for tables, spaces and newlines, respectively. This suggests that SPAM is more likely to have fancy HTML formatting than HAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pXScvrE3oKr"
      },
      "source": [
        "Reverse the list, and now we have the \"hammiest\" words... (words most indicative of a legitimate email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnR6IcmF3oKr"
      },
      "outputs": [],
      "source": [
        "np.flip(spam_ham_words)[0:top_x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0OJFrq33oKr"
      },
      "source": [
        "**Look at the words that best distinguish SPAM and HAM:**\n",
        "1. How many of your words showed up in the SPAM and HAM top 200 predictive words?\n",
        "2. Are they what you would have expected?\n",
        "3. Based on this, what can you say about the differences between how people make prediction and ML algorithms make predictions?\n",
        "4. Does this make you more confident or less confident in ML predictions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgog1Tcg3oKr"
      },
      "source": [
        "**Discuss here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRBVr7773oKr"
      },
      "source": [
        "# 2) Bayesian Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aycL1C93oKr"
      },
      "source": [
        "In this problem, we'll be using the `ASIA` dataset, which showcases the reltionships between travel, smoking, etc. and the probabilty of having various conditions. We will be using the [pomegranate](https://pomegranate.readthedocs.io/en/latest/) library to handle the heavy lifting for Bayes Nets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt4t7iEI3oKr"
      },
      "source": [
        "![](./asia_data.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nDjVtV3oKr"
      },
      "source": [
        "## 2.1) Exploring Bayes Nets (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwcDLa-3oKr"
      },
      "source": [
        "First go here: https://www.bayesserver.com/examples/networks/asia\n",
        "\n",
        "Try checking different boxes and seeing how the model updates. When you check a box, you're \"given\" a specific value for that node. For example, checking \"True\" for \"Visit to Asia\" means the patient has visited Asia, but we don't know the other probabilities yet. The new probabilities are \"given\" that you've visited Asia.\n",
        "\n",
        "Now answer the following questions. For each one, first make a **prediction** about how the model will change, and the try it to see if you're right. Write down your prediction and then the actual answer. If your prediction differs than the actual answer, try and discuss why.\n",
        "\n",
        "**After each question, uncheck all boxes.**\n",
        "\n",
        "1. If you set the value of Visit to Asia, which nodes will update?\n",
        "2. If you set the value of XRay Result, which nodes with update?\n",
        "3. First set the value for Has Tuberculosis. If you then set the value for Visit to Asia, which nodes will update?\n",
        "4. First set the value for Tuberculosis or Cancer. If you then set the value for Dyspnea, which nodes will update?\n",
        "5. If you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "6. First set the value for Tuberculosis or Cancer. Now if you check the box for Has Tuberculosis, will Has Lung Cancer update?\n",
        "\n",
        "If you wish to understand more about conditional independence and D-Separation, go here:  https://www.youtube.com/watch?v=_R_RYn5KelA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh6nOQi03oKs"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrtNDwaW3oKs"
      },
      "source": [
        "## 2.2) Building the Bayes Net (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlDKW5G3oKs"
      },
      "source": [
        "Just like in class, we can define the initial structure and conditional probability tables for the Bayes Net using our expert knowledge of the scenario (in this case, given to use by experts).\n",
        "\n",
        "For example, the first table gives the probability of having TB give that you have (or have not) visited Asia.\n",
        "\n",
        "| Asia | HasTB | P(HasTB\\|Asia) |\n",
        "| ---- | ----- | ------------- |\n",
        "| T | T | 0.05 |\n",
        "| T | F | 0.95 |\n",
        "| F | T | 0.01 |\n",
        "| F | F | 0.99 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToHUzunj3oKs"
      },
      "outputs": [],
      "source": [
        "from pomegranate import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTqq_CrP3oKs"
      },
      "outputs": [],
      "source": [
        "# First, we define our top level nodes with their base probabilities.\n",
        "\n",
        "visit_to_asia = DiscreteDistribution({'T':0.01, 'F':0.99})\n",
        "smoke = DiscreteDistribution({'T':0.5, 'F':0.5})\n",
        "\n",
        "# Now, we have to fill in all of the conditional probability tables for the other nodes\n",
        "\n",
        "has_tb = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Asia? #HasTB #Probability\n",
        "        [\"T\",\"T\",0.05],\n",
        "        [\"T\",\"F\",0.95],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99],\n",
        "    ], [visit_to_asia])\n",
        "\n",
        "\n",
        "has_lung_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke? \n",
        "        [\"T\",\"T\",0.1],\n",
        "        [\"T\",\"F\",0.9],\n",
        "        \n",
        "        [\"F\",\"T\",0.01],\n",
        "        [\"F\",\"F\",0.99]\n",
        "    ], [smoke])\n",
        "\n",
        "\n",
        "has_bc = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Smoke?\n",
        "        [\"T\",\"T\",0.6],\n",
        "        [\"T\",\"F\",0.4],\n",
        "        \n",
        "        [\"F\",\"T\",0.3],\n",
        "        [\"F\",\"F\",0.7]\n",
        "    ], [smoke])\n",
        "\n",
        "tb_or_cancer = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #Lung? TB? \n",
        "        [\"T\",\"T\",\"T\",1],\n",
        "        [\"T\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",1],\n",
        "        [\"T\",\"F\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",1],\n",
        "        [\"F\",\"T\",\"F\",0],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0],\n",
        "        [\"F\",\"F\",\"F\",1]\n",
        "    ], [has_lung_cancer,has_tb])\n",
        "\n",
        "x_ray_abnormal = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #TB or Cancer?\n",
        "        [\"T\",\"T\",0.98],\n",
        "        [\"T\",\"F\",0.02],\n",
        "        \n",
        "        [\"F\",\"T\",0.05],\n",
        "        [\"F\",\"F\",0.95]\n",
        "    ], [tb_or_cancer])\n",
        "\n",
        "dyspnea = ConditionalProbabilityTable(\n",
        "    [\n",
        "        #BC\n",
        "        [\"T\",\"T\",\"T\",0.9],\n",
        "        [\"T\",\"T\",\"F\",0.1],\n",
        "        \n",
        "        [\"T\",\"F\",\"T\",0.8],\n",
        "        [\"T\",\"F\",\"F\",0.2],\n",
        "        \n",
        "        [\"F\",\"T\",\"T\",0.7],\n",
        "        [\"F\",\"T\",\"F\",0.3],\n",
        "        \n",
        "        [\"F\",\"F\",\"T\",0.1],\n",
        "        [\"F\",\"F\",\"F\",0.9]\n",
        "    ], [has_bc, tb_or_cancer])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGmUvsL53oKs"
      },
      "outputs": [],
      "source": [
        "# Next we have to create all the nodes\n",
        "asia_node = Node(visit_to_asia, name=\"asia\")\n",
        "tb_node = Node(has_tb, name=\"tb\")\n",
        "smoke_node = Node(smoke, name=\"smoke\")\n",
        "lung_node = Node(has_lung_cancer, name=\"lung\")\n",
        "bronc_node = Node(has_bc, name=\"bc\")\n",
        "either_node = Node(tb_or_cancer, name=\"either\")\n",
        "xray_node = Node(x_ray_abnormal,name=\"xray\")\n",
        "dysp_node = Node(dyspnea, name=\"dysp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S40oXQvn3oKs"
      },
      "outputs": [],
      "source": [
        "# Now we init the model\n",
        "model = BayesianNetwork(\"ASIA\")\n",
        "model.add_states(asia_node,\n",
        "                 tb_node,\n",
        "                 smoke_node,\n",
        "                 lung_node,\n",
        "                 bronc_node,\n",
        "                 either_node,\n",
        "                 xray_node,\n",
        "                 dysp_node)\n",
        "\n",
        "# Add all of the correct edges \n",
        "model.add_edge(asia_node, tb_node)\n",
        "\n",
        "model.add_edge(smoke_node, bronc_node)\n",
        "model.add_edge(smoke_node, lung_node)\n",
        "\n",
        "model.add_edge(tb_node,either_node)\n",
        "model.add_edge(lung_node,either_node)\n",
        "\n",
        "model.add_edge(either_node, xray_node)\n",
        "model.add_edge(either_node, dysp_node)\n",
        "\n",
        "model.add_edge(bronc_node, dysp_node)\n",
        "\n",
        "# And then commit our changes\n",
        "model.bake()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoO5th503oKs"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the model structure\n",
        "def print_model_structure(model, features):\n",
        "    for i in range(len(features)):\n",
        "        parents = [features[pi] for pi in model.structure[i]]\n",
        "        print(f'Node \"{features[i]}\" has parents: {parents}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OSXGErC3oKs"
      },
      "outputs": [],
      "source": [
        "# We'll keep our features in this order for consistency\n",
        "features = [\n",
        "    \"Visit to Asia\",\n",
        "    \"Has TB\",\n",
        "    \"Smoker\",\n",
        "    \"Has Lung Cancer\",\n",
        "    \"Has Bronchitis\",\n",
        "    \"TB or Cancer\",\n",
        "    \"XRay Abnormal\",\n",
        "    \"Dyspnea\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ5vxsyM3oKs"
      },
      "outputs": [],
      "source": [
        "# Let's make sure the structure of our newly created model is correct\n",
        "print_model_structure(model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPamBaC3oKs"
      },
      "source": [
        "## 2.3) Predictions (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdmmYfwg3oKs"
      },
      "source": [
        "`predict` allows us to do inference based off of the data. It chooses the values that are the most likely.\n",
        "\n",
        "For example, let's say we have a patient who has an abnormal X-ray, but is not a smoker and hasn't visited Asia. We can then infer the most likely values for all of the other variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Old8Bu-33oKt"
      },
      "outputs": [],
      "source": [
        "model.predict([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJwMizsQ3oKt"
      },
      "outputs": [],
      "source": [
        "# Now let's say they *were* a smoker. See how it changes?\n",
        "model.predict([\n",
        "    [\"F\",None,\"T\",None,None,None,\"T\",None]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVPgog63oKt"
      },
      "source": [
        "We can get a little more detail and check out the actual probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXCd4FWR3oKt"
      },
      "outputs": [],
      "source": [
        "def pretty_results(results):\n",
        "    for i,dist in enumerate(results):\n",
        "        print(features[i])\n",
        "        if isinstance(dist,str):\n",
        "            print(dist)\n",
        "        else:\n",
        "            print(dist.parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo8xV7iX3oKt"
      },
      "outputs": [],
      "source": [
        "res = model.predict_proba([\n",
        "    [\"F\",None,\"F\",None,None,None,\"T\",None]\n",
        "])\n",
        "pretty_results(res[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyXMsO-3oKt"
      },
      "source": [
        "Use the Bayes net to calcualte the following probabilities:\n",
        "\n",
        "1. $P(xray=true | TBorCancer=true)$\n",
        "2. $P(xray=true | TBorCancer=true, TB=true)$\n",
        "3. $P(TB=true)$\n",
        "4. $P(TB=true | smoke=false)$\n",
        "5. $P(TB=true | smoke=false, TBorCancer=true)$\n",
        "\n",
        "What values are equivalent? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA2KHRPl3oKt"
      },
      "source": [
        "**Write your answers here:**\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "5.\n",
        "\n",
        "What values are equivalent? Why?\n",
        "\n",
        "And write code below to help you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBK7Kl-Y3oKt"
      },
      "outputs": [],
      "source": [
        "# As a reminder, here are the indices of the features\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXtRvJ363oKt"
      },
      "outputs": [],
      "source": [
        "# You can modify this code to help you\n",
        "pretty_results(model.predict_proba([\n",
        "    [None,None,'F',None,None,'T',None,None]\n",
        "])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvFDNx3H3oKt"
      },
      "source": [
        "## 2.4) Evaluating Bayes Nets (Follow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dOhNguE3oKt"
      },
      "source": [
        "Let's see how well this net is on inferencing from data. We're going to remove the Bronchitis column from this dataset, and see if our net can predict what the missing value should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzPG51iF3oKt"
      },
      "outputs": [],
      "source": [
        "# Some data we will use to generate our probabilities\n",
        "asia_data = pd.read_csv(\"Asia10k.csv\")\n",
        "asia_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_23SlnAX3oKu"
      },
      "outputs": [],
      "source": [
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTJ5Cqjn3oKu"
      },
      "outputs": [],
      "source": [
        "# Let's make sure we're consistant with our labels\n",
        "asia_data = asia_data.replace(\"no\", \"F\").replace(\"yes\", \"T\")\n",
        "asia_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9y4ltbBJ3oKu"
      },
      "outputs": [],
      "source": [
        "values = asia_data.values.copy()\n",
        "indices = np.random.choice(asia_data.index, 1000)\n",
        "values = values[indices]\n",
        "values[:,4] = None\n",
        "values[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DJTLoG03oKu"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMAVAWm3oKu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDkwBqa3oKu"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(asia_data.values[indices,4],np.array(predictions)[:,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj4hBVGX3oKu"
      },
      "outputs": [],
      "source": [
        "print(classification_report(asia_data.values[indices,4],np.array(predictions)[:,4]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHJubZb3oKu"
      },
      "source": [
        "## 2.5) Fitting a Bayes Net to Data (Group)\n",
        "\n",
        "In many applications, you may have a general idea of the structure of the Bayes Net, but do not have a list of probabilities. Luckily, given some data, we can fill out the probabilities in a given net. **Note:** You may only get similar results to the previous method, since it turns out this data was *simulated* from the given conditional probabilities. So, one would expect that the model would learn parameters like the ones we've given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83gWfLiD3oKu"
      },
      "outputs": [],
      "source": [
        "fitted_model = model.fit(asia_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_0JlNi3oKu"
      },
      "outputs": [],
      "source": [
        "# Helper function to print the probability distributions\n",
        "def print_distributions(model):\n",
        "    for i, state in enumerate(fitted_model.states):\n",
        "        print(features[i])\n",
        "        states = state.distribution.parameters[0]\n",
        "        if len(states)>1:\n",
        "            for state in states:\n",
        "                print(state)\n",
        "        #print(state.distribution.parameters[0])\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_8lKDiQ3oKu"
      },
      "outputs": [],
      "source": [
        "print_distributions(fitted_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB0ccZug3oKu"
      },
      "source": [
        "**Take a look at the learned probability distributions. Are they similar to \"expert\" ones given in the previous problem?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8751vMuM3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlViQF7V3oKv"
      },
      "source": [
        "Now, perform the same evaluation that you did in the previous problem. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNI7EMf53oKv"
      },
      "outputs": [],
      "source": [
        "# Remove Some other column other than Bronchitis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOdZ0cKE3oKv"
      },
      "outputs": [],
      "source": [
        "# Make the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdqbnXB23oKv"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0lXJgal3oKv"
      },
      "outputs": [],
      "source": [
        "# Classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIPgf3l13oKv"
      },
      "source": [
        "How well did it perform?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQWm4yT3oKv"
      },
      "source": [
        "## 2.6) Learning Structure from Data (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIlFb6YZ3oKv"
      },
      "source": [
        "Now, the most interesting problem is when we only have data, but we don't know the structure of the data (however, we still have a reason to believe that *it can be reperesented as Bayes Net*). Luckily, pomegranate has the ability to solve this problem as well. Given a dataset, we can use `from_samples` to build a Bayes net, structure and all, from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQw2ci3r3oKv"
      },
      "outputs": [],
      "source": [
        "learned_model = BayesianNetwork.from_samples(asia_data, algorithm='exact')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_afVm0o43oKv"
      },
      "outputs": [],
      "source": [
        "print_model_structure(learned_model, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyEpFS_3oKv"
      },
      "source": [
        "**Compare the model structure from experts vs learned from the data:**\n",
        "\n",
        "1. Draw out both models (the one you made earlier and the one learned) on a piece of paper.\n",
        "2. What are the differences you observe?\n",
        "3. Why might a model learned from the data have a different structure? Are some influences (edges) in the model more or less important than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnzxe7sv3oKv"
      },
      "source": [
        "**Discuss Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7GIWIFG3oKw"
      },
      "source": [
        "## 2.7) BYOB: Build Your Own Bayes Net (Group, if time permits)\n",
        "If you find yourself with some extra time after this portion, consider **building a Bayes net that represents something in your daily life**. It could be the effect of traffic on a morning commute, deciding what to do for dinner, etc. It can be very small, only around 3-5 nodes probably (conditional probability tables are a pain!). Then play around with predictions and probabilities to see how various factors impact your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzIWCRVT3oKw"
      },
      "outputs": [],
      "source": [
        "# Start building!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}